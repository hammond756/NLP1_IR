{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.daniweb.com/programming/software-development/code/216839/number-to-word-converter-python\n",
    "def int2word(n):\n",
    "    \"\"\"\n",
    "    convert an integer number n into a string of english words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return any string that is not all digits\n",
    "    if not all([char.isdigit() for char in n]):\n",
    "        return n\n",
    "    \n",
    "    # break the number into groups of 3 digits using slicing\n",
    "    # each group representing hundred, thousand, million, billion, ...\n",
    "    n3 = []\n",
    "    r1 = \"\"\n",
    "    # create numeric string\n",
    "    ns = str(n)\n",
    "    for k in range(3, 33, 3):\n",
    "        r = ns[-k:]\n",
    "        q = len(ns) - k\n",
    "        # break if end of ns has been reached\n",
    "        if q < -2:\n",
    "            break\n",
    "        else:\n",
    "            if  q >= 0:\n",
    "                n3.append(int(r[:3]))\n",
    "            elif q >= -1:\n",
    "                n3.append(int(r[:2]))\n",
    "            elif q >= -2:\n",
    "                n3.append(int(r[:1]))\n",
    "        r1 = r\n",
    "    \n",
    "    #print n3  # test\n",
    "    \n",
    "    # break each group of 3 digits into\n",
    "    # ones, tens/twenties, hundreds\n",
    "    # and form a string\n",
    "    nw = \"\"\n",
    "    for i, x in enumerate(n3):\n",
    "        b1 = x % 10\n",
    "        b2 = (x % 100)//10\n",
    "        b3 = (x % 1000)//100\n",
    "        #print b1, b2, b3  # test\n",
    "        if x == 0:\n",
    "            continue  # skip\n",
    "        else:\n",
    "            t = thousands[i]\n",
    "        if b2 == 0:\n",
    "            nw = ones[b1] + t + nw\n",
    "        elif b2 == 1:\n",
    "            nw = tens[b1] + t + nw\n",
    "        elif b2 > 1:\n",
    "            nw = twenties[b2] + ones[b1] + t + nw\n",
    "        if b3 > 0:\n",
    "            nw = ones[b3] + \"hundred \" + nw\n",
    "    return nw.strip().split()\n",
    "\n",
    "############# globals ################\n",
    "ones = [\"\", \"one \",\"two \",\"three \",\"four \", \"five \",\n",
    "    \"six \",\"seven \",\"eight \",\"nine \"]\n",
    "tens = [\"ten \",\"eleven \",\"twelve \",\"thirteen \", \"fourteen \",\n",
    "    \"fifteen \",\"sixteen \",\"seventeen \",\"eighteen \",\"nineteen \"]\n",
    "twenties = [\"\",\"\",\"twenty \",\"thirty \",\"forty \",\n",
    "    \"fifty \",\"sixty \",\"seventy \",\"eighty \",\"ninety \"]\n",
    "thousands = [\"\",\"thousand \",\"million \", \"billion \", \"trillion \",\n",
    "    \"quadrillion \", \"quintillion \", \"sextillion \", \"septillion \",\"octillion \",\n",
    "    \"nonillion \", \"decillion \", \"undecillion \", \"duodecillion \", \"tredecillion \",\n",
    "    \"quattuordecillion \", \"quindecillion\", \"sexdecillion \", \"septendecillion \", \n",
    "    \"octodecillion \", \"novemdecillion \", \"vigintillion \"]\n",
    "\n",
    "def digits_to_text(document):\n",
    "    digits_to_text = []\n",
    "    for token in document:\n",
    "        temp = int2word(token)\n",
    "        if type(temp) is list:\n",
    "            digits_to_text.extend(temp)\n",
    "        else:\n",
    "            digits_to_text.append(temp)\n",
    "\n",
    "    return digits_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import heapq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# TODO: find scientific reference that also claims Snowball is better\n",
    "# alternatively: http://www.nltk.org/howto/stem.html claims this already.\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "\n",
    "# check if stopword corpus is available on your system\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    _ = WordNetLemmatizer().lemmatize('test')\n",
    "except:\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "# Embeddings don't work well for words that occur < 5 times\n",
    "THRESHOLD = 5\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "def hide_infrequent_words(document, threshold):\n",
    "    counter = Counter(document)\n",
    "    new_document = []\n",
    "    \n",
    "    for word in document:\n",
    "        if counter[word] > threshold:\n",
    "            new_document.append(word)\n",
    "    \n",
    "    return new_document\n",
    "\n",
    "def filter_document(document):\n",
    "    \"\"\"Filter list of words based on some conventional methods, like removing stopwords and\n",
    "    lemmatization\"\"\"\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words.update(punctuation)\n",
    "    document = list(filter(lambda x: x not in stop_words, document))\n",
    "\n",
    "    # [I, am, 34] -> [I, am, thirty, four]\n",
    "    document = digits_to_text(document)\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    document = list(map(lemmatizer.lemmatize, document))\n",
    "\n",
    "    return document\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, json_data, image_features, img2feat, transform=None):\n",
    "        \n",
    "            \n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.corpus = filter_document(self.corpus)\n",
    "        self.corpus = hide_infrequent_words(self.corpus, THRESHOLD)\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        \n",
    "        self.vocab.append(UNK)\n",
    "        \n",
    "        self.w2i = {word : i for i, word in enumerate(self.vocab)}\n",
    "        self.w2i = defaultdict(lambda: self.w2i[UNK], self.w2i)\n",
    "        \n",
    "        with open(img2feat, 'r') as f:\n",
    "            img2feat = json.load(f)['IR_imgid2id']\n",
    "        img_features = np.asarray(h5py.File(image_features, 'r')['img_features'])\n",
    "        self.prepareData(img_features, img2feat)\n",
    "        \n",
    "    def prepareData (self, img_features, img2feat):\n",
    "        all_img_features = []\n",
    "        all_qa_vectors = []\n",
    "        all_caption_vector = []\n",
    "        all_dialog_vector = []\n",
    "        for idx in range(len(self)):\n",
    "            item = self.json_data.iloc[idx]\n",
    "            image_features = [torch.FloatTensor(img_features[idx]) for idx in map(lambda x: img2feat[str(x)], item.img_list)]\n",
    "            qa_vectors = [self.make_context_vector(line[0].split()) for line in item.dialog]\n",
    "#             qa_embeddings = [sum(self.embeddings(Variable(qa_vector)).data) for qa_vector in qa_vectors]\n",
    "            caption_vector = self.make_context_vector(item.caption.split())\n",
    "#             caption_embedding = sum(self.embeddings(Variable(caption_vector)).data)\n",
    "#             qa_embeddings.append(caption_embedding)\n",
    "            dialog_vector = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog_vector.extend(item.caption.split(' '))\n",
    "            dialog_vector = self.make_context_vector(dialog_vector)\n",
    "            \n",
    "            all_img_features.append(image_features)\n",
    "            all_qa_vectors.append(qa_vectors)\n",
    "            all_caption_vector.append(caption_vector)\n",
    "            all_dialog_vector.append(dialog_vector)\n",
    "            \n",
    "        self.json_data['img_features'] = all_img_features\n",
    "        self.json_data['qa_vectors'] = all_qa_vectors\n",
    "        self.json_data['caption_vector'] = all_caption_vector\n",
    "        self.json_data['dialog_vector'] = all_dialog_vector\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = []\n",
    "        for idx in range(len(self)):\n",
    "            item = self.json_data.iloc[idx]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "\n",
    "            words.append(dialog)\n",
    "            \n",
    "        return list(chain.from_iterable(words))\n",
    "    \n",
    "    def make_context_vector(self, context):\n",
    "        idxs = [self.w2i[w] for w in context]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return self.json_data.iloc[key]\n",
    "#         if isinstance(key, slice):\n",
    "#             #Get the start, stop, and step from the slice\n",
    "#             return [self[ii] for ii in range(*key.indices(len(self)))]\n",
    "#         elif isinstance(key, int):\n",
    "#             if key < 0 : #Handle negative indices\n",
    "#                 key += len( self )\n",
    "#             if key < 0 or key >= len(self) :\n",
    "#                 raise IndexError(\"The index ({}) is out of range.\".format(key))\n",
    "            \n",
    "#             item = self.json_data.iloc[key]\n",
    "\n",
    "#             # Flatten dialog and add caption into 1d array\n",
    "#             dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "#             dialog.extend(item.caption.split(' '))\n",
    "#             dialog = filter_document(dialog)\n",
    "#             dialog = self.make_context_vector(dialog)\n",
    "\n",
    "#             img_ids = np.array(item.img_list)\n",
    "#             img_features = [self.img_features[idx] for idx in map(lambda x: self.img2feat[str(x)], img_ids)]\n",
    "#             img_features = np.array(img_features)\n",
    "#             img_features = torch.FloatTensor(img_features)\n",
    "\n",
    "#             target = item.target\n",
    "#             target = torch.LongTensor(np.array([target]))\n",
    "\n",
    "#             if torch.cuda.is_available():\n",
    "#                 dialog, img_features, target = dialog.cuda(), img_features.cuda(), target.cuda()\n",
    "                \n",
    "#             return dialog, img_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>dialog</th>\n",
       "      <th>img_list</th>\n",
       "      <th>target</th>\n",
       "      <th>target_img_id</th>\n",
       "      <th>img_features</th>\n",
       "      <th>qa_vectors</th>\n",
       "      <th>caption_vector</th>\n",
       "      <th>dialog_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a person that is laying next to a dog</td>\n",
       "      <td>[[is this a child or adult ? adult], [male or ...</td>\n",
       "      <td>[540370, 142686, 200718, 325806, 412575, 55676...</td>\n",
       "      <td>7</td>\n",
       "      <td>378466</td>\n",
       "      <td>[[0.22707563638687134, 0.23830120265483856, 0....</td>\n",
       "      <td>[[6282, 6282, 6282, 6134, 6282, 982, 6282, 982...</td>\n",
       "      <td>[6282, 446, 6282, 6282, 5589, 727, 6282, 6282,...</td>\n",
       "      <td>[6282, 6282, 6282, 6134, 6282, 982, 6282, 982,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a black and white photo of a man on a horse by...</td>\n",
       "      <td>[[what color is horse ? brown, but it's black ...</td>\n",
       "      <td>[502588, 414700, 509350, 303597, 575029, 43960...</td>\n",
       "      <td>4</td>\n",
       "      <td>575029</td>\n",
       "      <td>[[1.5016311407089233, 0.18418647348880768, 0.7...</td>\n",
       "      <td>[[6282, 1188, 6282, 662, 6282, 5570, 6282, 271...</td>\n",
       "      <td>[6282, 778, 6282, 3969, 4475, 6282, 6282, 5030...</td>\n",
       "      <td>[6282, 1188, 6282, 662, 6282, 5570, 6282, 2717...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a cheese and pepperoni pizza in an oven</td>\n",
       "      <td>[[is oven hot ? yes], [do u see red flames ? n...</td>\n",
       "      <td>[227779, 36324, 256451, 60043, 226587, 351705,...</td>\n",
       "      <td>1</td>\n",
       "      <td>36324</td>\n",
       "      <td>[[0.06620161980390549, 0.3357774317264557, 1.9...</td>\n",
       "      <td>[[6282, 1139, 3668, 6282, 2472], [6282, 2878, ...</td>\n",
       "      <td>[6282, 4045, 6282, 181, 2708, 6282, 6282, 1139]</td>\n",
       "      <td>[6282, 1139, 3668, 6282, 2472, 6282, 2878, 157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>an image of a kids playing with a frisbee</td>\n",
       "      <td>[[how many kids are there ? 2], [what color is...</td>\n",
       "      <td>[234316, 197327, 474780, 247744, 456678, 14682...</td>\n",
       "      <td>9</td>\n",
       "      <td>100012</td>\n",
       "      <td>[[1.5324183702468872, 0.2816334664821625, 0.51...</td>\n",
       "      <td>[[6282, 947, 6282, 6282, 6282, 6282, 6282], [6...</td>\n",
       "      <td>[6282, 1800, 6282, 6282, 6282, 3385, 6282, 628...</td>\n",
       "      <td>[6282, 947, 6282, 6282, 6282, 6282, 6282, 6282...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>2 people in a field throwing a frisbee at a a ...</td>\n",
       "      <td>[[are the people female or male ? male], [abou...</td>\n",
       "      <td>[147838, 316269, 240195, 304038, 6146, 132638,...</td>\n",
       "      <td>6</td>\n",
       "      <td>219663</td>\n",
       "      <td>[[0.5365859270095825, 0.39938515424728394, 1.2...</td>\n",
       "      <td>[[6282, 6282, 6098, 1235, 6282, 899, 6282, 899...</td>\n",
       "      <td>[6282, 6098, 6282, 6282, 1059, 1663, 6282, 159...</td>\n",
       "      <td>[6282, 6282, 6098, 1235, 6282, 899, 6282, 899,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                caption  \\\n",
       "0                 a person that is laying next to a dog   \n",
       "1     a black and white photo of a man on a horse by...   \n",
       "10              a cheese and pepperoni pizza in an oven   \n",
       "100           an image of a kids playing with a frisbee   \n",
       "1000  2 people in a field throwing a frisbee at a a ...   \n",
       "\n",
       "                                                 dialog  \\\n",
       "0     [[is this a child or adult ? adult], [male or ...   \n",
       "1     [[what color is horse ? brown, but it's black ...   \n",
       "10    [[is oven hot ? yes], [do u see red flames ? n...   \n",
       "100   [[how many kids are there ? 2], [what color is...   \n",
       "1000  [[are the people female or male ? male], [abou...   \n",
       "\n",
       "                                               img_list  target  \\\n",
       "0     [540370, 142686, 200718, 325806, 412575, 55676...       7   \n",
       "1     [502588, 414700, 509350, 303597, 575029, 43960...       4   \n",
       "10    [227779, 36324, 256451, 60043, 226587, 351705,...       1   \n",
       "100   [234316, 197327, 474780, 247744, 456678, 14682...       9   \n",
       "1000  [147838, 316269, 240195, 304038, 6146, 132638,...       6   \n",
       "\n",
       "      target_img_id                                       img_features  \\\n",
       "0            378466  [[0.22707563638687134, 0.23830120265483856, 0....   \n",
       "1            575029  [[1.5016311407089233, 0.18418647348880768, 0.7...   \n",
       "10            36324  [[0.06620161980390549, 0.3357774317264557, 1.9...   \n",
       "100          100012  [[1.5324183702468872, 0.2816334664821625, 0.51...   \n",
       "1000         219663  [[0.5365859270095825, 0.39938515424728394, 1.2...   \n",
       "\n",
       "                                             qa_vectors  \\\n",
       "0     [[6282, 6282, 6282, 6134, 6282, 982, 6282, 982...   \n",
       "1     [[6282, 1188, 6282, 662, 6282, 5570, 6282, 271...   \n",
       "10    [[6282, 1139, 3668, 6282, 2472], [6282, 2878, ...   \n",
       "100   [[6282, 947, 6282, 6282, 6282, 6282, 6282], [6...   \n",
       "1000  [[6282, 6282, 6098, 1235, 6282, 899, 6282, 899...   \n",
       "\n",
       "                                         caption_vector  \\\n",
       "0     [6282, 446, 6282, 6282, 5589, 727, 6282, 6282,...   \n",
       "1     [6282, 778, 6282, 3969, 4475, 6282, 6282, 5030...   \n",
       "10      [6282, 4045, 6282, 181, 2708, 6282, 6282, 1139]   \n",
       "100   [6282, 1800, 6282, 6282, 6282, 3385, 6282, 628...   \n",
       "1000  [6282, 6098, 6282, 6282, 1059, 1663, 6282, 159...   \n",
       "\n",
       "                                          dialog_vector  \n",
       "0     [6282, 6282, 6282, 6134, 6282, 982, 6282, 982,...  \n",
       "1     [6282, 1188, 6282, 662, 6282, 5570, 6282, 2717...  \n",
       "10    [6282, 1139, 3668, 6282, 2472, 6282, 2878, 157...  \n",
       "100   [6282, 947, 6282, 6282, 6282, 6282, 6282, 6282...  \n",
       "1000  [6282, 6282, 6098, 1235, 6282, 899, 6282, 899,...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "EASY_1000 = ['Data', 'Easy', 'IR_train_easy_1000.json']\n",
    "VAL_200 = ['Data', 'Easy', 'IR_val_easy_200.json']\n",
    "VALID_EASY = ['Data', 'Easy', 'IR_val_easy.json']\n",
    "IMG_FEATURES = ['Data', 'Features', 'IR_image_features.h5']\n",
    "INDEX_MAP = ['Data', 'Features', 'IR_img_features2id.json']\n",
    "\n",
    "IMG_SIZE = 2048\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "dialog_data = DialogDataset(os.path.join(*TRAIN_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "valid_data = DialogDataset(os.path.join(*VALID_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "\n",
    "vocab_size = len(dialog_data.vocab)\n",
    "print(len(dialog_data[0:3])) # can now slice this bitch up\n",
    "dialog_data.json_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, output_dim)\n",
    "        self.activation_function2 = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, text_module, text_dim, img_size):\n",
    "        super(MaxEnt, self).__init__()\n",
    "\n",
    "        self.text_module = text_module\n",
    "        \n",
    "        self.inputDimension = text_dim + img_size\n",
    "        self.hiddenDimension = 20\n",
    "        self.outputDimension = 1\n",
    "        self.numberOfLayers = 2\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.inputDimension, \n",
    "            hidden_size=self.hiddenDimension,\n",
    "            num_layers=self.numberOfLayers,\n",
    "            dropout=0.05\n",
    "        )\n",
    "\n",
    "        self.hidden = self.init_hidden(1)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hiddenDimension, self.outputDimension)\n",
    "#         self.softmax = nn.LogSoftmax()\n",
    "\n",
    "        # for weightTensor in self.parameters():\n",
    "        # \tweightTensor.data = torch.zeros(weightTensor.size())\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden1 = torch.zeros(self.numberOfLayers, batch_size, self.hiddenDimension)\n",
    "        hidden2 = torch.zeros(self.numberOfLayers, batch_size, self.hiddenDimension)\n",
    "        if torch.cuda.is_available():\n",
    "            hidden1, hidden2 = hidden1.cuda(), hidden2.cuda()\n",
    "        return (Variable(hidden1),Variable(hidden2))\n",
    "        \n",
    "    def prepare (self, item):\n",
    "        list_of_string_vectors = []\n",
    "        list_of_string_vectors.append(item.caption_vector)\n",
    "        list_of_string_vectors.extend(item.qa_vectors)\n",
    "        if torch.cuda.is_available():\n",
    "            list_of_string_vectors = [vec.cuda() for vec in list_of_string_vectors]\n",
    "        text_features = [self.text_module(Variable(vec)) for vec in list_of_string_vectors]\n",
    "        text_features = torch.squeeze(torch.stack(text_features))\n",
    "#         print(text_features.size())\n",
    "        sequences = text_features.repeat(len(item.img_features), 1, 1)\n",
    "#         print(sequences.size())\n",
    "        img_add_on = torch.stack(item.img_features).repeat(text_features.size(0), 1, 1).permute(1,0,2)\n",
    "        if torch.cuda.is_available():\n",
    "            img_add_on = img_add_on.cuda()\n",
    "#         print(img_add_on.size())\n",
    "        sequences = torch.cat((sequences, Variable(img_add_on)), 2)\n",
    "#         print(sequences.size())\n",
    "        target = torch.LongTensor(np.array([item.target]))\n",
    "        if torch.cuda.is_available():\n",
    "            target = target.cuda()\n",
    "        target = Variable(target)\n",
    "        \n",
    "        self.hidden = self.init_hidden(len(item.img_features))\n",
    "        return sequences.permute(1,0,2), target\n",
    "    \n",
    "    def prepareBatch (self, batch):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for dialog, imgFeatures, target in batch:\n",
    "            inputs.append(self.prepare(dialog, imgFeatures))\n",
    "            targets.append(target)\n",
    "        inputs = torch.cat(inputs)\n",
    "        targets = torch.cat(targets)\n",
    "        return Variable(inputs), Variable(targets)\n",
    "        \n",
    "    def forward(self, inp, batch_size):\n",
    "#         print(inp.size())\n",
    "        lstmOut, self.hidden = self.lstm(inp, self.hidden)\n",
    "#         print(lstmOut.size())\n",
    "        scores = self.linear(lstmOut).view(batch_size, -1)\n",
    "#         print(scores.size())\n",
    "        scores = F.log_softmax(scores.transpose(0,1)).transpose(0,1)\n",
    "#         print(scores.size())\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya ya\n",
      "cuda ready bitches\n"
     ]
    }
   ],
   "source": [
    "TEXT_DIM = 512\n",
    "\n",
    "cbow_model = CBOW(vocab_size, EMBEDDING_DIM, TEXT_DIM)\n",
    "model = MaxEnt(cbow_model, TEXT_DIM, IMG_SIZE)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ya ya\")\n",
    "    cbow_model = cbow_model.cuda()\n",
    "    model = model.cuda()\n",
    "    print(\"cuda ready bitches\")\n",
    "else:\n",
    "    print(\"no no\")\n",
    "    \n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "epochs_trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2991711592674253"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(model, data, loss_func):\n",
    "    total_loss = 0\n",
    "    item_count = 0\n",
    "    for i, item in data.iterrows():\n",
    "        inputs, target = model.prepare(item)\n",
    "        pred = model(inputs, len(item.img_features))\n",
    "        pred = pred[:,-1].unsqueeze(0)\n",
    "        loss = loss_func(pred, target)\n",
    "        total_loss += loss.data[0]\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "def predict(model, data):\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    for i, item in data.iterrows():\n",
    "        \n",
    "        inputs, target = model.prepare(item)\n",
    "        \n",
    "        # For top 1:\n",
    "        pred = model(inputs, len(item.img_features))\n",
    "        pred = pred[:,-1].unsqueeze(0)\n",
    "        img, idx = torch.max(pred, 1)\n",
    "\n",
    "        if idx.data[0] == target.data[0]:\n",
    "            correct_top1 += 1\n",
    "        \n",
    "        # For top 5:\n",
    "        pred = pred.data.cpu().numpy().flatten()\n",
    "        top_5 = heapq.nlargest(5, range(len(pred)), pred.__getitem__)\n",
    "        if target.data[0] in top_5:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    return correct_top1 / len(data), correct_top5 / len(data)\n",
    "\n",
    "validate(model, valid_data[:100], nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_console(i, n_epochs, batch_size, batch_per_epoch, error, start_time, processing_speed):\n",
    "    percentOfEpc = (i / batch_per_epoch) * 100\n",
    "    print(\"{:.0f}s:\\t epoch: {}\\t batch:{} ({:.1f}%) \\t training error: {:.6f}\\t speed: {:.1f} dialogs/s\"\n",
    "          .format(timer() - start_time, \n",
    "                  n_epochs, \n",
    "                  i, \n",
    "                  percentOfEpc, \n",
    "                  error, \n",
    "                  processing_speed))\n",
    "    \n",
    "def init_stats_log(label, training_portion, validation_portion, embeddings_dim, epochs, batch_count):\n",
    "    timestr = time.strftime(\"%m-%d-%H-%M\")\n",
    "    filename = \"{}-t_size_{}-v_size_{}-emb_{}-eps_{}-dt_{}-batch_{}.txt\".format(label,\n",
    "                                                                       training_portion,\n",
    "                                                                       validation_portion,\n",
    "                                                                       EMBEDDING_DIM,\n",
    "                                                                       epochs,\n",
    "                                                                       timestr,\n",
    "                                                                       batch_count)\n",
    "\n",
    "    target_path = ['Training_recordings', filename]\n",
    "    stats_log = open(os.path.join(*target_path), 'w')\n",
    "    stats_log.write(\"EPOCH|AVG_LOSS|TOT_LOSS|VAL_ERROR|CORRECT_TOP1|CORRECT_TOP5\\n\")\n",
    "    print(\"Logging enabled in:\", filename)\n",
    "    \n",
    "    return stats_log, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have: 40000 dialogs, batch size of 1 with 1 as remainder to offset batches each epoch\n",
      "Logging enabled in: test_top1_top2-t_size_40000-v_size_5000-emb_50-eps_100-dt_12-17-00-22-batch_1.txt\n",
      "3s:\t epoch: 0\t batch:185 (0.5%) \t training error: 2.318327\t speed: 61.4 dialogs/s\n",
      "6s:\t epoch: 0\t batch:374 (0.9%) \t training error: 2.310816\t speed: 62.1 dialogs/s\n",
      "9s:\t epoch: 0\t batch:562 (1.4%) \t training error: 2.308304\t speed: 62.2 dialogs/s\n",
      "12s:\t epoch: 0\t batch:749 (1.9%) \t training error: 2.306941\t speed: 62.2 dialogs/s\n",
      "15s:\t epoch: 0\t batch:936 (2.3%) \t training error: 2.305598\t speed: 62.2 dialogs/s\n",
      "18s:\t epoch: 0\t batch:1124 (2.8%) \t training error: 2.304885\t speed: 62.2 dialogs/s\n",
      "21s:\t epoch: 0\t batch:1312 (3.3%) \t training error: 2.304650\t speed: 62.3 dialogs/s\n",
      "24s:\t epoch: 0\t batch:1499 (3.7%) \t training error: 2.304394\t speed: 62.2 dialogs/s\n",
      "27s:\t epoch: 0\t batch:1686 (4.2%) \t training error: 2.304210\t speed: 62.2 dialogs/s\n",
      "30s:\t epoch: 0\t batch:1872 (4.7%) \t training error: 2.304084\t speed: 62.2 dialogs/s\n",
      "33s:\t epoch: 0\t batch:2058 (5.1%) \t training error: 2.303942\t speed: 62.2 dialogs/s\n",
      "36s:\t epoch: 0\t batch:2247 (5.6%) \t training error: 2.303833\t speed: 62.2 dialogs/s\n",
      "39s:\t epoch: 0\t batch:2433 (6.1%) \t training error: 2.303974\t speed: 62.2 dialogs/s\n",
      "42s:\t epoch: 0\t batch:2623 (6.6%) \t training error: 2.303876\t speed: 62.3 dialogs/s\n",
      "45s:\t epoch: 0\t batch:2796 (7.0%) \t training error: 2.303791\t speed: 61.9 dialogs/s\n",
      "48s:\t epoch: 0\t batch:2983 (7.5%) \t training error: 2.303732\t speed: 62.0 dialogs/s\n",
      "51s:\t epoch: 0\t batch:3170 (7.9%) \t training error: 2.303671\t speed: 62.0 dialogs/s\n",
      "54s:\t epoch: 0\t batch:3356 (8.4%) \t training error: 2.303600\t speed: 62.0 dialogs/s\n",
      "57s:\t epoch: 0\t batch:3543 (8.9%) \t training error: 2.303568\t speed: 62.0 dialogs/s\n",
      "60s:\t epoch: 0\t batch:3731 (9.3%) \t training error: 2.303537\t speed: 62.0 dialogs/s\n",
      "63s:\t epoch: 0\t batch:3917 (9.8%) \t training error: 2.303490\t speed: 62.0 dialogs/s\n",
      "66s:\t epoch: 0\t batch:4105 (10.3%) \t training error: 2.303452\t speed: 62.0 dialogs/s\n",
      "69s:\t epoch: 0\t batch:4290 (10.7%) \t training error: 2.303418\t speed: 62.0 dialogs/s\n",
      "72s:\t epoch: 0\t batch:4474 (11.2%) \t training error: 2.303385\t speed: 62.0 dialogs/s\n",
      "75s:\t epoch: 0\t batch:4663 (11.7%) \t training error: 2.303362\t speed: 62.0 dialogs/s\n",
      "78s:\t epoch: 0\t batch:4851 (12.1%) \t training error: 2.303335\t speed: 62.0 dialogs/s\n",
      "81s:\t epoch: 0\t batch:5037 (12.6%) \t training error: 2.303318\t speed: 62.0 dialogs/s\n",
      "84s:\t epoch: 0\t batch:5227 (13.1%) \t training error: 2.303259\t speed: 62.1 dialogs/s\n",
      "87s:\t epoch: 0\t batch:5415 (13.5%) \t training error: 2.303250\t speed: 62.1 dialogs/s\n",
      "90s:\t epoch: 0\t batch:5604 (14.0%) \t training error: 2.303221\t speed: 62.1 dialogs/s\n",
      "93s:\t epoch: 0\t batch:5787 (14.5%) \t training error: 2.303173\t speed: 62.0 dialogs/s\n",
      "96s:\t epoch: 0\t batch:5974 (14.9%) \t training error: 2.303160\t speed: 62.0 dialogs/s\n",
      "99s:\t epoch: 0\t batch:6158 (15.4%) \t training error: 2.303152\t speed: 62.0 dialogs/s\n",
      "102s:\t epoch: 0\t batch:6339 (15.8%) \t training error: 2.303189\t speed: 61.9 dialogs/s\n",
      "105s:\t epoch: 0\t batch:6518 (16.3%) \t training error: 2.303208\t speed: 61.9 dialogs/s\n",
      "108s:\t epoch: 0\t batch:6698 (16.7%) \t training error: 2.303220\t speed: 61.8 dialogs/s\n",
      "111s:\t epoch: 0\t batch:6880 (17.2%) \t training error: 2.303219\t speed: 61.8 dialogs/s\n",
      "114s:\t epoch: 0\t batch:7068 (17.7%) \t training error: 2.303214\t speed: 61.8 dialogs/s\n",
      "117s:\t epoch: 0\t batch:7254 (18.1%) \t training error: 2.303209\t speed: 61.8 dialogs/s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 # batching is not possible with LSTM model since we use batching for different images\n",
    "numEpochs = 100\n",
    "learningRate = 1e-2\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "start_time = timer()\n",
    "lastPrintTime = start_time\n",
    "\n",
    "continueFromEpc = 0\n",
    "continueFromI = 0\n",
    "sampleCount = len(dialog_data)\n",
    "batchCountPerEpc = int(sampleCount/batch_size)-1\n",
    "remainderCount = sampleCount - batchCountPerEpc * batch_size\n",
    "print(\"we have: {} dialogs, batch size of {} with {} as remainder to offset batches each epoch\".format(sampleCount, batch_size, remainderCount))\n",
    "offset = 0\n",
    "\n",
    "logging = True\n",
    "\n",
    "training_portion = len(dialog_data)\n",
    "validation_portion = len(valid_data)\n",
    "\n",
    "if logging == True:\n",
    "    stats_log, filename = init_stats_log(\"test_top1_top2\", \n",
    "                               training_portion,\n",
    "                               validation_portion,\n",
    "                               EMBEDDING_DIM,\n",
    "                               numEpochs,\n",
    "                               batch_size)\n",
    "\n",
    "else:\n",
    "    print(\"Logging disabled!\")\n",
    "    filename = \"\"\n",
    "\n",
    "for t in range(numEpochs):\n",
    "    lastPrintTime = timer()\n",
    "    epoch_start_time = timer()\n",
    "    \n",
    "    total_loss = 0\n",
    "    updates = 0\n",
    "    \n",
    "    if t == 0 and continueFromI > 0:\n",
    "        # continue where I crashed\n",
    "        print(\"continuing\")\n",
    "        model.load_state_dict(torch.load('maxent_{}epc_{}iter.pt'.format(continueFromEpc, continueFromI+1)))\n",
    "    \n",
    "    for i in range(continueFromI, batchCountPerEpc):\n",
    "        \n",
    "               \n",
    "        batchBegin = offset + i * batch_size\n",
    "        batchEnd = batchBegin + batch_size\n",
    "        \n",
    "        item = dialog_data[i]\n",
    "        \n",
    "        inputs, target = model.prepare(item)\n",
    "        pred = model(inputs, len(item.img_features))\n",
    "        pred = pred[:,-1].unsqueeze(0)\n",
    "        loss = criterion(pred, target)\n",
    "        \n",
    "        training_errors.append(loss.data[0])\n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if timer()  - lastPrintTime > 3:\n",
    "            processing_speed = (i*batch_size) / (timer() - epoch_start_time)\n",
    "            log_to_console(i, epochs_trained, batch_size, batchCountPerEpc, total_loss / i, start_time, processing_speed)\n",
    "            lastPrintTime = timer()\n",
    "    print(\"{:.1f}s:\\t Finished epoch. Calculating test error..\".format(timer() - start_time))\n",
    "    \n",
    "    avg_loss = total_loss / batchCountPerEpc\n",
    "    top_1_score, top_5_score = predict(model, valid_data)\n",
    "    validation_error = validate(model, valid_data, criterion)\n",
    "    \n",
    "    if logging == True:\n",
    "        stats_log.write(\"{}|{}|{}|{}|{}|{}\\n\".format(epochs_trained, avg_loss, total_loss, validation_error, top_1_score, top_5_score))\n",
    "            \n",
    "    epochs_trained += 1\n",
    "    offset = (offset + 1) % remainderCount\n",
    "    print(\"{:.1f}s:\\t test error: {:.6f}\".format(timer() - start_time, validation_error))\n",
    "    continueFromI = 0\n",
    "    if epochs_trained % 10 == 0:\n",
    "        fileName = \"maxent_{}batch_{}epc.pt\".format(batch_size, epochs_trained)\n",
    "        torch.save(model.state_dict(), fileName)\n",
    "        print(\"saved\\t\", fileName)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    dialog, image, target = dialog_data[i]\n",
    "    inputs = model.prepare(dialog, image)\n",
    "    pred = model(Variable(inputs))\n",
    "    print(pred)\n",
    "\n",
    "if logging == True:\n",
    "    stats_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(filename):\n",
    "    \n",
    "    \n",
    "    # Read file and data\n",
    "    with open(\"Training_recordings/\" + filename, 'r') as f:\n",
    "        data = [x.strip() for x in f.readlines()] \n",
    "    \n",
    "    data = np.array([line.split(\"|\") for line in data[1:]]).T\n",
    "    \n",
    "    epochs, avg_loss, total_loss, val_error, correct_top_1, correct_top_5 = data\n",
    "    \n",
    "    epochs = np.array(epochs, dtype=np.int8)\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(epochs, np.array(avg_loss, dtype=np.float32), '.-')\n",
    "    plt.title('average loss, validation error and correct predictions')\n",
    "    plt.ylabel('Average\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(epochs, np.array(val_error, dtype=np.float32), '-')\n",
    "    plt.ylabel('Validation\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(epochs, np.array(correct_top_1, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 1')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(epochs, np.array(correct_top_5, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 5')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_graph(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
