{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.daniweb.com/programming/software-development/code/216839/number-to-word-converter-python\n",
    "def int2word(n):\n",
    "    \"\"\"\n",
    "    convert an integer number n into a string of english words\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return any string that is not all digits\n",
    "    if not all([char.isdigit() for char in n]):\n",
    "        return n\n",
    "    \n",
    "    # break the number into groups of 3 digits using slicing\n",
    "    # each group representing hundred, thousand, million, billion, ...\n",
    "    n3 = []\n",
    "    r1 = \"\"\n",
    "    # create numeric string\n",
    "    ns = str(n)\n",
    "    for k in range(3, 33, 3):\n",
    "        r = ns[-k:]\n",
    "        q = len(ns) - k\n",
    "        # break if end of ns has been reached\n",
    "        if q < -2:\n",
    "            break\n",
    "        else:\n",
    "            if  q >= 0:\n",
    "                n3.append(int(r[:3]))\n",
    "            elif q >= -1:\n",
    "                n3.append(int(r[:2]))\n",
    "            elif q >= -2:\n",
    "                n3.append(int(r[:1]))\n",
    "        r1 = r\n",
    "    \n",
    "    #print n3  # test\n",
    "    \n",
    "    # break each group of 3 digits into\n",
    "    # ones, tens/twenties, hundreds\n",
    "    # and form a string\n",
    "    nw = \"\"\n",
    "    for i, x in enumerate(n3):\n",
    "        b1 = x % 10\n",
    "        b2 = (x % 100)//10\n",
    "        b3 = (x % 1000)//100\n",
    "        #print b1, b2, b3  # test\n",
    "        if x == 0:\n",
    "            continue  # skip\n",
    "        else:\n",
    "            t = thousands[i]\n",
    "        if b2 == 0:\n",
    "            nw = ones[b1] + t + nw\n",
    "        elif b2 == 1:\n",
    "            nw = tens[b1] + t + nw\n",
    "        elif b2 > 1:\n",
    "            nw = twenties[b2] + ones[b1] + t + nw\n",
    "        if b3 > 0:\n",
    "            nw = ones[b3] + \"hundred \" + nw\n",
    "    return nw.strip().split()\n",
    "\n",
    "############# globals ################\n",
    "ones = [\"\", \"one \",\"two \",\"three \",\"four \", \"five \",\n",
    "    \"six \",\"seven \",\"eight \",\"nine \"]\n",
    "tens = [\"ten \",\"eleven \",\"twelve \",\"thirteen \", \"fourteen \",\n",
    "    \"fifteen \",\"sixteen \",\"seventeen \",\"eighteen \",\"nineteen \"]\n",
    "twenties = [\"\",\"\",\"twenty \",\"thirty \",\"forty \",\n",
    "    \"fifty \",\"sixty \",\"seventy \",\"eighty \",\"ninety \"]\n",
    "thousands = [\"\",\"thousand \",\"million \", \"billion \", \"trillion \",\n",
    "    \"quadrillion \", \"quintillion \", \"sextillion \", \"septillion \",\"octillion \",\n",
    "    \"nonillion \", \"decillion \", \"undecillion \", \"duodecillion \", \"tredecillion \",\n",
    "    \"quattuordecillion \", \"quindecillion\", \"sexdecillion \", \"septendecillion \", \n",
    "    \"octodecillion \", \"novemdecillion \", \"vigintillion \"]\n",
    "\n",
    "def digits_to_text(document):\n",
    "    digits_to_text = []\n",
    "    for token in document:\n",
    "        temp = int2word(token)\n",
    "        if type(temp) is list:\n",
    "            digits_to_text.extend(temp)\n",
    "        else:\n",
    "            digits_to_text.append(temp)\n",
    "\n",
    "    return digits_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import heapq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# TODO: find scientific reference that also claims Snowball is better\n",
    "# alternatively: http://www.nltk.org/howto/stem.html claims this already.\n",
    "from nltk.stem import SnowballStemmer, PorterStemmer\n",
    "\n",
    "# check if stopword corpus is available on your system\n",
    "try:\n",
    "    _ = stopwords.words('english')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "try:\n",
    "    _ = WordNetLemmatizer().lemmatize('test')\n",
    "except:\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "# Embeddings don't work well for words that occur < 5 times\n",
    "THRESHOLD = 5\n",
    "UNK = \"<unk>\"\n",
    "\n",
    "def hide_infrequent_words(document, threshold):\n",
    "    counter = Counter(document)\n",
    "    new_document = []\n",
    "    \n",
    "    for word in document:\n",
    "        if counter[word] > threshold:\n",
    "            new_document.append(word)\n",
    "    \n",
    "    return new_document\n",
    "\n",
    "def filter_document(document):\n",
    "    \"\"\"Filter list of words based on some conventional methods, like removing stopwords and\n",
    "    lemmatization\"\"\"\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    stop_words.update(punctuation)\n",
    "    document = list(filter(lambda x: x not in stop_words, document))\n",
    "\n",
    "    # [I, am, 34] -> [I, am, thirty, four]\n",
    "    document = digits_to_text(document)\n",
    "\n",
    "    # Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    document = list(map(lemmatizer.lemmatize, document))\n",
    "\n",
    "    return document\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, json_data, image_features, img2feat, transform=None):\n",
    "        \n",
    "        with open(img2feat, 'r') as f:\n",
    "            self.img2feat = json.load(f)['IR_imgid2id']\n",
    "            \n",
    "        self.img_features = np.asarray(h5py.File(image_features, 'r')['img_features'])\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.corpus = filter_document(self.corpus)\n",
    "        self.corpus = hide_infrequent_words(self.corpus, THRESHOLD)\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        \n",
    "        self.vocab.append(UNK)\n",
    "        \n",
    "        self.w2i = {word : i for i, word in enumerate(self.vocab)}\n",
    "        self.w2i = defaultdict(lambda: self.w2i[UNK], self.w2i)\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = []\n",
    "        for idx in range(len(self)):\n",
    "            item = self.json_data.iloc[idx]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "\n",
    "            words.append(dialog)\n",
    "            \n",
    "        return list(chain.from_iterable(words))\n",
    "    \n",
    "    def make_context_vector(self, context):\n",
    "        idxs = [self.w2i[w] for w in context]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            #Get the start, stop, and step from the slice\n",
    "            return [self[ii] for ii in range(*key.indices(len(self)))]\n",
    "        elif isinstance(key, int):\n",
    "            if key < 0 : #Handle negative indices\n",
    "                key += len( self )\n",
    "            if key < 0 or key >= len(self) :\n",
    "                raise IndexError(\"The index ({}) is out of range.\".format(key))\n",
    "            \n",
    "            item = self.json_data.iloc[key]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "            dialog = filter_document(dialog)\n",
    "            dialog = self.make_context_vector(dialog)\n",
    "\n",
    "            img_ids = np.array(item.img_list)\n",
    "            img_features = [self.img_features[idx] for idx in map(lambda x: self.img2feat[str(x)], img_ids)]\n",
    "            img_features = np.array(img_features)\n",
    "            img_features = torch.FloatTensor(img_features)\n",
    "\n",
    "            target = item.target\n",
    "            target = torch.LongTensor(np.array([target]))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                dialog, img_features, target = dialog.cuda(), img_features.cuda(), target.cuda()\n",
    "                \n",
    "            return dialog, img_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "EASY_1000 = ['Data', 'Easy', 'IR_train_easy_1000.json']\n",
    "VAL_200 = ['Data', 'Easy', 'IR_val_easy_200.json']\n",
    "VALID_EASY = ['Data', 'Easy', 'IR_val_easy.json']\n",
    "IMG_FEATURES = ['Data', 'Features', 'IR_image_features.h5']\n",
    "INDEX_MAP = ['Data', 'Features', 'IR_img_features2id.json']\n",
    "\n",
    "IMG_SIZE = 2048\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "dialog_data = DialogDataset(os.path.join(*EASY_1000), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "valid_data = DialogDataset(os.path.join(*VAL_200), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "\n",
    "vocab_size = len(dialog_data.vocab)\n",
    "print(len(dialog_data[0:3])) # can now slice this bitch up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, output_dim)\n",
    "        self.activation_function2 = nn.ReLU()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, text_module, text_dim, img_size):\n",
    "        super(MaxEnt, self).__init__()\n",
    "\n",
    "        self.text_module = text_module\n",
    "        self.linear = nn.Linear(text_dim + img_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 1)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def prepare (self, dialog, imgFeatures):\n",
    "        text_features = self.text_module(Variable(dialog))\n",
    "        text_features = text_features.expand(imgFeatures.size(0), text_features.size(1))\n",
    "        concat = torch.cat((imgFeatures, text_features.data), 1)\n",
    "        return concat\n",
    "    \n",
    "    def prepareBatch (self, batch):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for dialog, imgFeatures, target in batch:\n",
    "            inputs.append(self.prepare(dialog, imgFeatures))\n",
    "            targets.append(target)\n",
    "        inputs = torch.cat(inputs)\n",
    "        targets = torch.cat(targets)\n",
    "        return Variable(inputs), Variable(targets)\n",
    "        \n",
    "    def forward(self, inp, batch_size=1):\n",
    "        scores = self.linear(inp)\n",
    "        scores = F.relu(scores)\n",
    "        scores = self.linear2(scores).view(batch_size, -1)\n",
    "        scores = self.softmax(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no no\n",
      "CPU times: user 5.21 ms, sys: 1.13 ms, total: 6.34 ms\n",
      "Wall time: 14.6 ms\n",
      "CPU times: user 1.24 ms, sys: 174 µs, total: 1.42 ms\n",
      "Wall time: 1.04 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.4174 -2.1616 -2.1697 -2.2547 -2.3385 -2.4374 -2.3006 -2.3270 -2.3197 -2.3368\n",
       "[torch.FloatTensor of size 1x10]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_DIM = 512\n",
    "\n",
    "cbow_model = CBOW(vocab_size, EMBEDDING_DIM, TEXT_DIM)\n",
    "model = MaxEnt(cbow_model, TEXT_DIM, IMG_SIZE)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ya ya\")\n",
    "    cbow_model = cbow_model.cuda()\n",
    "    model = model.cuda()\n",
    "    print(\"cuda ready bitches\")\n",
    "else:\n",
    "    print(\"no no\")\n",
    "    \n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "epochs_trained = 0\n",
    "\n",
    "dialog, images, target = dialog_data[0]\n",
    "%time inputs = model.prepare(dialog, images)\n",
    "%time model(Variable(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2880350001415803"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(model, data, loss_func):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (dialog, imgFeatures, target) in enumerate(data):\n",
    "        inputs = model.prepare(dialog, imgFeatures)\n",
    "        \n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "        \n",
    "        pred = model(inputs)\n",
    "        \n",
    "        loss = loss_func(pred, target)\n",
    "        total_loss += loss.data[0]\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "def predict(model, data):\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    for i, (dialog, img_features, target) in enumerate(data):\n",
    "        \n",
    "        inputs = model.prepare(dialog, img_features)\n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "        \n",
    "        # For top 1:\n",
    "        pred = model(inputs)\n",
    "        img, idx = torch.max(pred, 1)\n",
    "\n",
    "        if idx.data[0] == target.data[0]:\n",
    "            correct_top1 += 1\n",
    "        \n",
    "        # For top 5:\n",
    "        pred = pred.data.cpu().numpy().flatten()\n",
    "        top_5 = heapq.nlargest(5, range(len(pred)), pred.__getitem__)\n",
    "        if target.data[0] in top_5:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    return correct_top1 / len(data), correct_top5 / len(data)\n",
    "\n",
    "validate(model, valid_data, nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_console(i, n_epochs, batch_size, batch_per_epoch, error, start_time, processing_speed):\n",
    "    percentOfEpc = (i / batch_per_epoch) * 100\n",
    "    print(\"{:.0f}s:\\t epoch: {}\\t batch:{} ({:.1f}%) \\t training error: {:.6f}\\t speed: {:.1f} dialogs/s\"\n",
    "          .format(timer() - start_time, \n",
    "                  n_epochs, \n",
    "                  i, \n",
    "                  percentOfEpc, \n",
    "                  error, \n",
    "                  processing_speed))\n",
    "    \n",
    "def init_stats_log(label, training_portion, validation_portion, embeddings_dim, epochs, batch_count, learning_rate):\n",
    "    timestr = time.strftime(\"%m-%d-%H-%M\")\n",
    "    filename = \"{}-t_size_{}-v_size_{}-emb_{}-eps_{}-dt_{}-batch_{}-lr_{}.txt\".format(label,\n",
    "                                                                       training_portion,\n",
    "                                                                       validation_portion,\n",
    "                                                                       EMBEDDING_DIM,\n",
    "                                                                       epochs,\n",
    "                                                                       timestr,\n",
    "                                                                       batch_count,\n",
    "                                                                       learning_rate)\n",
    "\n",
    "    target_path = ['Training_recordings', filename]\n",
    "    stats_log = open(os.path.join(*target_path), 'w')\n",
    "    stats_log.write(\"EPOCH|AVG_LOSS|TOT_LOSS|VAL_ERROR|CORRECT_TOP1|CORRECT_TOP5\\n\")\n",
    "    print(\"Logging enabled in:\", filename)\n",
    "    \n",
    "    return stats_log, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have: 1000 dialogs, batch size of 10 with 10 as remainder to offset batches each epoch\n",
      "Logging enabled in: test_top1_top2-t_size_1000-v_size_201-emb_100-eps_25-dt_12-16-22-17-batch_10-lr_0.0001.txt\n",
      "3s:\t epoch: 126\t batch:36 (36.4%) \t training error: 0.000847\t speed: 119.5 dialogs/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-494990b3a99b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdialog_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchBegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatchEnd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepareBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1c4e7999b29c>\u001b[0m in \u001b[0;36mprepareBatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1c4e7999b29c>\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, dialog, imgFeatures)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgFeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-bbc20bc18557>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mod__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mIndexSelect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m# else fall through and raise an error in Index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i, index)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "numEpochs = 25\n",
    "learningRate = 1e-4\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "start_time = timer()\n",
    "lastPrintTime = start_time\n",
    "\n",
    "continueFromEpc = 0\n",
    "continueFromI = 0\n",
    "sampleCount = len(dialog_data)\n",
    "batchCountPerEpc = int(sampleCount/batch_size)-1\n",
    "remainderCount = sampleCount - batchCountPerEpc * batch_size\n",
    "print(\"we have: {} dialogs, batch size of {} with {} as remainder to offset batches each epoch\".format(sampleCount, batch_size, remainderCount))\n",
    "offset = 0\n",
    "\n",
    "logging = True\n",
    "\n",
    "training_portion = len(dialog_data)\n",
    "validation_portion = len(valid_data)\n",
    "\n",
    "if logging == True:\n",
    "    stats_log, filename = init_stats_log(\"test_top1_top2\", \n",
    "                               training_portion,\n",
    "                               validation_portion,\n",
    "                               EMBEDDING_DIM,\n",
    "                               numEpochs,\n",
    "                               batch_size,\n",
    "                               learningRate)\n",
    "\n",
    "else:\n",
    "    print(\"Logging disabled!\")\n",
    "    filename = \"\"\n",
    "\n",
    "for t in range(numEpochs):\n",
    "    lastPrintTime = timer()\n",
    "    epoch_start_time = timer()\n",
    "    \n",
    "    total_loss = 0\n",
    "    updates = 0\n",
    "    \n",
    "    if t == 0 and continueFromI > 0:\n",
    "        # continue where I crashed\n",
    "        print(\"continuing\")\n",
    "        model.load_state_dict(torch.load('maxent_{}epc_{}iter.pt'.format(continueFromEpc, continueFromI+1)))\n",
    "    \n",
    "    for i in range(continueFromI, batchCountPerEpc):\n",
    "        \n",
    "        # In case of RNN, clear hidden state\n",
    "        #model.hidden = steerNet.init_hidden(batch_size)\n",
    "        \n",
    "        batchBegin = offset + i * batch_size\n",
    "        batchEnd = batchBegin + batch_size\n",
    "        \n",
    "        batch = dialog_data[batchBegin:batchEnd]\n",
    "        inputs, targets = model.prepareBatch(batch)\n",
    "        \n",
    "        predictions = model(inputs, batch_size)\n",
    "        \n",
    "        loss = criterion(predictions, targets)\n",
    "        training_errors.append(loss.data[0])\n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if timer()  - lastPrintTime > 3:\n",
    "            processing_speed = (i*batch_size) / (timer() - epoch_start_time)\n",
    "            log_to_console(i, epochs_trained, batch_size, batchCountPerEpc, total_loss / i, start_time, processing_speed)\n",
    "            lastPrintTime = timer()\n",
    "    print(\"{:.1f}s:\\t Finished epoch. Calculating test error..\".format(timer() - start_time))\n",
    "    \n",
    "    avg_loss = total_loss / batchCountPerEpc\n",
    "    top_1_score, top_5_score = predict(model, valid_data)\n",
    "    validation_error = validate(model, valid_data, criterion)\n",
    "    \n",
    "    if logging == True:\n",
    "        stats_log.write(\"{}|{}|{}|{}|{}|{}\\n\".format(epochs_trained, avg_loss, total_loss, validation_error, top_1_score, top_5_score))\n",
    "            \n",
    "    epochs_trained += 1\n",
    "    offset = (offset + 1) % remainderCount\n",
    "    print()\n",
    "    print(\"<--------------->\")\n",
    "    print(\"{:.1f}s:\\t top-1: \\t {:.2f} \\t top-5: \\t {:.2f} \\t test error: {:.6f}\".format(timer() - start_time, top_1_score, top_5_score, validation_error))\n",
    "    print(\"<--------------->\")\n",
    "    print()\n",
    "    continueFromI = 0\n",
    "\n",
    "if logging == True:\n",
    "    stats_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_graph(filename):\n",
    "    \n",
    "    \n",
    "    # Read file and data\n",
    "    with open(\"Training_recordings/\" + filename, 'r') as f:\n",
    "        data = [x.strip() for x in f.readlines()] \n",
    "    \n",
    "    data = np.array([line.split(\"|\") for line in data[1:]]).T\n",
    "    \n",
    "    epochs, avg_loss, total_loss, val_error, correct_top_1, correct_top_5 = data\n",
    "    \n",
    "    epochs = np.array(epochs, dtype=np.int8)\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(epochs, np.array(avg_loss, dtype=np.float32), '.-')\n",
    "    plt.title('average loss, validation error and correct predictions')\n",
    "    plt.ylabel('Average\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(epochs, np.array(val_error, dtype=np.float32), '-')\n",
    "    plt.ylabel('Validation\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(epochs, np.array(correct_top_1, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 1')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(epochs, np.array(correct_top_5, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 5')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_graph(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
