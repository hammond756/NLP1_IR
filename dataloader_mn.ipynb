{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "import heapq\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, json_data, image_features, img2feat, transform=None):\n",
    "        \n",
    "        with open(img2feat, 'r') as f:\n",
    "            self.img2feat = json.load(f)['IR_imgid2id']\n",
    "            \n",
    "        self.img_features = np.asarray(h5py.File(image_features, 'r')['img_features'])\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        self.vocab.append(PAD)\n",
    "        self.w2i = {word : i for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = []\n",
    "        for idx in range(len(self)):\n",
    "            item = self.json_data.iloc[idx]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "\n",
    "            words.append(dialog)\n",
    "            \n",
    "        return list(chain.from_iterable(words))\n",
    "    \n",
    "    def make_context_vector(self, context):\n",
    "        idxs = [self.w2i[w] for w in context]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        return tensor\n",
    "    \n",
    "    def convert_to_idx(self, sequence):\n",
    "        return [self.w2i[w] for w in sequence]\n",
    "    \n",
    "    def pad(self, dialog):\n",
    "        # length of longest question/answer pair\n",
    "        n = max(map(len, dialog))\n",
    "        return [sentence + [PAD] * (n - len(sentence)) for sentence in dialog]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        item = self.json_data.iloc[key]\n",
    "        \n",
    "        diag = item.dialog\n",
    "        capt = [item.caption]\n",
    "\n",
    "        # No appending required if it is already done before:\n",
    "        if diag[-1] != capt:\n",
    "            diag.append(capt)\n",
    "        diag = [QA[0].split() for QA in diag]\n",
    "        diag = self.pad(diag)\n",
    "        \n",
    "        try:\n",
    "            diag = [self.convert_to_idx(QA) for QA in diag]\n",
    "        except:\n",
    "            print(diag)\n",
    "\n",
    "        diag = torch.LongTensor(diag)\n",
    "\n",
    "        img_ids = np.array(item.img_list)\n",
    "        img_features = [self.img_features[idx] for idx in map(lambda x: self.img2feat[str(x)], img_ids)]\n",
    "        img_features = np.array(img_features)\n",
    "        img_features = torch.FloatTensor(img_features)\n",
    "\n",
    "        target = item.target\n",
    "        target = torch.LongTensor(np.array([target]))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            diag, img_features, target = diag.cuda(), img_features.cuda(), target.cuda()\n",
    "\n",
    "        return diag, img_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       " \n",
       " Columns 0 to 12 \n",
       "   119  2890   347  1752  3332    85  3757    85  4515  4515  4515  4515  4515\n",
       "  4003  3332  4004  3757  4003  4515  4515  4515  4515  4515  4515  4515  4515\n",
       "  1742  3516  1262  3332  4048  3757  1262  4515  4515  4515  4515  4515  4515\n",
       "  1742  3516  2613  3077  2816  1778  3757  3105  3306  1605   119   347  1838\n",
       "   119  2816  1778  2895  3332  2642  3757   890   119  2920  4515  4515  4515\n",
       "  2532   516   119  2816  1838  3757  3920   560   695  4515  4515  4515  4515\n",
       "  2532   516   119  2816  2920  3757  3473  3920  4515  4515  4515  4515  4515\n",
       "  2532  1514   119  2816  1042  3757  2327  4515  4515  4515  4515  4515  4515\n",
       "  1155  2816  1042  3945  4168   560  1817  3757  3436  4515  4515  4515  4515\n",
       "  2532   516   119  2816  1042  3757  4098  4515  4515  4515  4515  4515  4515\n",
       "   347   919  1818   119  2613  2941   162   347  1042  4515  4515  4515  4515\n",
       " \n",
       " Columns 13 to 18 \n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "   150  1615  1512   560  2816  1778\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       "  4515  4515  4515  4515  4515  4515\n",
       " [torch.LongTensor of size 11x19], \n",
       "  0.2271  0.2383  0.4295  ...   1.0665  1.0852  0.3636\n",
       "  0.1314  0.2542  1.7184  ...   0.5293  0.7909  0.1379\n",
       "  0.2266  0.1438  1.9781  ...   0.2134  0.0481  0.0479\n",
       "           ...             â‹±             ...          \n",
       "  0.0050  0.3181  1.8139  ...   0.1029  0.6153  0.2048\n",
       "  2.5743  0.2204  1.0295  ...   0.0671  0.1832  0.1447\n",
       "  3.5460  0.1536  0.8659  ...   0.2847  0.3697  0.5222\n",
       " [torch.FloatTensor of size 10x2048], \n",
       "  7\n",
       " [torch.LongTensor of size 1])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "EASY_1000 = ['Data', 'Easy', 'IR_train_easy_1000.json']\n",
    "VAL_200 = ['Data', 'Easy', 'IR_val_easy_200.json']\n",
    "VALID_EASY = ['Data', 'Easy', 'IR_val_easy.json']\n",
    "IMG_FEATURES = ['Data', 'Features', 'IR_image_features.h5']\n",
    "INDEX_MAP = ['Data', 'Features', 'IR_img_features2id.json']\n",
    "\n",
    "IMG_SIZE = 2048\n",
    "EMBEDDING_DIM = 5\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "dialog_data = DialogDataset(os.path.join(*EASY_1000), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "valid_data = DialogDataset(os.path.join(*VAL_200), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "\n",
    "vocab_size = len(dialog_data.vocab)\n",
    "dialog_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, output_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, output_dim)\n",
    "        self.activation_function2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs, batch_size = 1):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = self.embeddings(inputs)\n",
    "        sum_dim = 1 if batch_size > 1 else 0\n",
    "        embeds = torch.sum(embeds, sum_dim)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, text_module, vocab_size, img_size):\n",
    "        super(MaxEnt, self).__init__()\n",
    "\n",
    "        self.text_module = text_module\n",
    "        self.linear = nn.Linear(vocab_size + img_size, 1)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def prepare (self, dialog, imgFeatures):\n",
    "        text_features = self.text_module(Variable(dialog))\n",
    "        text_features = text_features.expand(imgFeatures.size(0), text_features.size(1))\n",
    "        concat = torch.cat((imgFeatures, text_features.data), 1)\n",
    "        return concat\n",
    "    \n",
    "    def prepareBatch (self, batch):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for dialog, imgFeatures, target in batch:\n",
    "            inputs.append(self.prepare(dialog, imgFeatures))\n",
    "            targets.append(target)\n",
    "        inputs = torch.cat(inputs)\n",
    "        targets = torch.cat(targets)\n",
    "        return Variable(inputs), Variable(targets)\n",
    "        \n",
    "    def forward(self, inp, batch_size = 1):\n",
    "        scores = self.linear(inp).view(batch_size, -1)\n",
    "        scores = self.softmax(scores)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemNet(nn.Module):\n",
    "    \"\"\"NB: output size of text_module should match memory_dim\"\"\"\n",
    "    def __init__(self, text_module, memory_dim, output_dim):\n",
    "        super(MemNet, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.memory_dim = memory_dim\n",
    "        \n",
    "        self.text_module = text_module\n",
    "        self.linear = nn.Linear(memory_dim, output_dim)\n",
    "    \n",
    "    def forward(self, dialog, img_features):\n",
    "        \n",
    "        scores = torch.FloatTensor(len(img_features)).zero_()\n",
    "        \n",
    "        for i, img_feature in enumerate(img_features):\n",
    "            history = self.text_module(dialog, batch_size = len(dialog))\n",
    "\n",
    "            # inner product of history and current image (normalized to prevent extreme values in softmax)\n",
    "            memory = history @ img_feature\n",
    "            memory = torch.div(memory, (torch.norm(memory)))\n",
    "            \n",
    "            weights = F.softmax(memory)\n",
    "\n",
    "            # Take weighted sum of history\n",
    "            history_vector = torch.mm(weights.unsqueeze(0), history).squeeze()\n",
    "            \n",
    "            print(history_vector)\n",
    "\n",
    "            out = self.linear(history_vector + img_feature)\n",
    "            scores[i] = F.sigmoid(out).data[0]\n",
    "        \n",
    "        scores = F.log_softmax(scores)\n",
    "        \n",
    "        return scores\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate CBOW here for sentence embeddings:\n",
    "torch.manual_seed(1)\n",
    "\n",
    "EMBEDDING_DIM = 5\n",
    "IMG_SIZE = 2048\n",
    "OUTPUT_DIM = IMG_SIZE # For simplicity...\n",
    "\n",
    "cbow_model = CBOW(vocab_size, EMBEDDING_DIM, OUTPUT_DIM)\n",
    "mem_net = MemNet(cbow_model, OUTPUT_DIM, 1)\n",
    "max_ent = MaxEnt(cbow_model, vocab_size, IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.3218\n",
      " 0.4076\n",
      " 0.3182\n",
      " 0.1017\n",
      " 0.2462\n",
      " 0.2959\n",
      " 0.3088\n",
      " 0.3268\n",
      " 0.2701\n",
      " 0.3438\n",
      " 0.2778\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1168\n",
      "  2.0008\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3487\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3218\n",
      " 0.4063\n",
      " 0.3194\n",
      " 0.1040\n",
      " 0.2482\n",
      " 0.2976\n",
      " 0.3083\n",
      " 0.3269\n",
      " 0.2684\n",
      " 0.3448\n",
      " 0.2748\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1166\n",
      "  2.0008\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3482\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3233\n",
      " 0.4072\n",
      " 0.3163\n",
      " 0.0988\n",
      " 0.2475\n",
      " 0.2976\n",
      " 0.3077\n",
      " 0.3250\n",
      " 0.2700\n",
      " 0.3428\n",
      " 0.2816\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1170\n",
      "  2.0010\n",
      "   â‹®    \n",
      "  0.2569\n",
      "  3.3489\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3224\n",
      " 0.4085\n",
      " 0.3176\n",
      " 0.1037\n",
      " 0.2462\n",
      " 0.2961\n",
      " 0.3085\n",
      " 0.3257\n",
      " 0.2687\n",
      " 0.3436\n",
      " 0.2788\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1173\n",
      "  2.0010\n",
      "   â‹®    \n",
      "  0.2571\n",
      "  3.3487\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3212\n",
      " 0.4095\n",
      " 0.3170\n",
      " 0.1028\n",
      " 0.2463\n",
      " 0.2943\n",
      " 0.3092\n",
      " 0.3263\n",
      " 0.2718\n",
      " 0.3422\n",
      " 0.2788\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1172\n",
      "  2.0007\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3488\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3226\n",
      " 0.4059\n",
      " 0.3177\n",
      " 0.1016\n",
      " 0.2470\n",
      " 0.2975\n",
      " 0.3082\n",
      " 0.3265\n",
      " 0.2691\n",
      " 0.3441\n",
      " 0.2793\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1167\n",
      "  2.0007\n",
      "   â‹®    \n",
      "  0.2571\n",
      "  3.3484\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3213\n",
      " 0.4055\n",
      " 0.3184\n",
      " 0.1041\n",
      " 0.2483\n",
      " 0.2972\n",
      " 0.3082\n",
      " 0.3275\n",
      " 0.2710\n",
      " 0.3438\n",
      " 0.2762\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1165\n",
      "  2.0002\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3478\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3229\n",
      " 0.4103\n",
      " 0.3162\n",
      " 0.1043\n",
      " 0.2489\n",
      " 0.2964\n",
      " 0.3076\n",
      " 0.3235\n",
      " 0.2686\n",
      " 0.3420\n",
      " 0.2799\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1179\n",
      "  2.0012\n",
      "   â‹®    \n",
      "  0.2567\n",
      "  3.3488\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3219\n",
      " 0.4093\n",
      " 0.3179\n",
      " 0.0978\n",
      " 0.2456\n",
      " 0.2947\n",
      " 0.3097\n",
      " 0.3269\n",
      " 0.2703\n",
      " 0.3433\n",
      " 0.2781\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1168\n",
      "  2.0014\n",
      "   â‹®    \n",
      "  0.2573\n",
      "  3.3497\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "\n",
      " 0.3219\n",
      " 0.4095\n",
      " 0.3183\n",
      " 0.1025\n",
      " 0.2462\n",
      " 0.2954\n",
      " 0.3090\n",
      " 0.3264\n",
      " 0.2689\n",
      " 0.3437\n",
      " 0.2767\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1172\n",
      "  2.0013\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3491\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MN SEQUENCE:\n",
    "# Best paper + implementation description I was able to find:\n",
    "# https://arxiv.org/pdf/1503.08895.pdf\n",
    "# Where 'Question' in our case is one of the 10 images (or all 10 in one go).\n",
    "\n",
    "# NOTE: all variable names are in accordance with section 2.1 from paper URL.\n",
    "\n",
    "# Trying simple forward prediction. Test if right probabilities are generated:\n",
    "\n",
    "# Set the number of memory items (1 caption + 10 QA's ):\n",
    "dialog, images, target = dialog_data[0]\n",
    "x = len(dialog)\n",
    "\n",
    "for i in images:\n",
    "    # Create an empty history matrix, [11 x 2048]\n",
    "    m = torch.FloatTensor(x, IMG_SIZE).zero_()\n",
    "\n",
    "\n",
    "    # calculate 'm' features for each exchange, a matrix of size 11 x 2048\n",
    "    for idx, sentence in enumerate(dialog):\n",
    "        m_i = cbow_model(Variable(sentence))\n",
    "        m[idx] = m_i.data\n",
    "\n",
    "    # m = m.expand()\n",
    "    # Preferably find 'u' img features, a matrix of [2048 x 10]. For now just take one image to \n",
    "    # resemble sect. 2.1 as good as possible. u = [2048 x 1]\n",
    "    u = i\n",
    "\n",
    "    # inner product of history matrix and img features, produces 11 x 10 matrix \n",
    "    # but for now 11 x 1 since u = [2048 x 1]\n",
    "    p = m @ u\n",
    "    p.div_(torch.norm(p))\n",
    "    \n",
    "    print(p)\n",
    "\n",
    "    # softmax the to get proper proabilities\n",
    "    p = F.softmax(p)\n",
    "    \n",
    "    o = torch.FloatTensor(x, IMG_SIZE).zero_()\n",
    "    # Weighted sum 'o', representation of memory output:\n",
    "    # p_i * c_i, where c_i is in our case equal to m_i\n",
    "    for idx, p_i in enumerate(p):\n",
    "        o[idx] = p_i.data * m[idx]\n",
    "\n",
    "    o = Variable(torch.sum(o, dim=0))\n",
    "\n",
    "\n",
    "    u = Variable(u)\n",
    "\n",
    "    print(o)\n",
    "\n",
    "    # create a \"Weight matrix\" W:\n",
    "    # W = nn.Linear(2048, 1)\n",
    "\n",
    "    # # Finally, push u + o:\n",
    "    # a = F.logsigmoid(W(u + o))\n",
    "    # print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.3218\n",
      " 0.4076\n",
      " 0.3182\n",
      " 0.1017\n",
      " 0.2462\n",
      " 0.2959\n",
      " 0.3088\n",
      " 0.3268\n",
      " 0.2701\n",
      " 0.3438\n",
      " 0.2778\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1168\n",
      "  2.0008\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3487\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3218\n",
      " 0.4063\n",
      " 0.3194\n",
      " 0.1040\n",
      " 0.2482\n",
      " 0.2976\n",
      " 0.3083\n",
      " 0.3269\n",
      " 0.2684\n",
      " 0.3448\n",
      " 0.2748\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1166\n",
      "  2.0008\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3482\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3233\n",
      " 0.4072\n",
      " 0.3163\n",
      " 0.0988\n",
      " 0.2475\n",
      " 0.2976\n",
      " 0.3077\n",
      " 0.3250\n",
      " 0.2700\n",
      " 0.3428\n",
      " 0.2816\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1170\n",
      "  2.0010\n",
      "   â‹®    \n",
      "  0.2569\n",
      "  3.3489\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3224\n",
      " 0.4085\n",
      " 0.3176\n",
      " 0.1037\n",
      " 0.2462\n",
      " 0.2961\n",
      " 0.3085\n",
      " 0.3257\n",
      " 0.2687\n",
      " 0.3436\n",
      " 0.2788\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1173\n",
      "  2.0010\n",
      "   â‹®    \n",
      "  0.2571\n",
      "  3.3487\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3212\n",
      " 0.4095\n",
      " 0.3170\n",
      " 0.1028\n",
      " 0.2463\n",
      " 0.2943\n",
      " 0.3092\n",
      " 0.3263\n",
      " 0.2718\n",
      " 0.3422\n",
      " 0.2788\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1172\n",
      "  2.0007\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3488\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3226\n",
      " 0.4059\n",
      " 0.3177\n",
      " 0.1016\n",
      " 0.2470\n",
      " 0.2975\n",
      " 0.3082\n",
      " 0.3265\n",
      " 0.2691\n",
      " 0.3441\n",
      " 0.2793\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1167\n",
      "  2.0007\n",
      "   â‹®    \n",
      "  0.2571\n",
      "  3.3484\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3213\n",
      " 0.4055\n",
      " 0.3184\n",
      " 0.1041\n",
      " 0.2483\n",
      " 0.2972\n",
      " 0.3082\n",
      " 0.3275\n",
      " 0.2710\n",
      " 0.3438\n",
      " 0.2762\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1165\n",
      "  2.0002\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3478\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3229\n",
      " 0.4103\n",
      " 0.3162\n",
      " 0.1043\n",
      " 0.2489\n",
      " 0.2964\n",
      " 0.3076\n",
      " 0.3235\n",
      " 0.2686\n",
      " 0.3420\n",
      " 0.2799\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1179\n",
      "  2.0012\n",
      "   â‹®    \n",
      "  0.2567\n",
      "  3.3488\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3219\n",
      " 0.4093\n",
      " 0.3179\n",
      " 0.0978\n",
      " 0.2456\n",
      " 0.2947\n",
      " 0.3097\n",
      " 0.3269\n",
      " 0.2703\n",
      " 0.3433\n",
      " 0.2781\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1168\n",
      "  2.0014\n",
      "   â‹®    \n",
      "  0.2573\n",
      "  3.3497\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.3219\n",
      " 0.4095\n",
      " 0.3183\n",
      " 0.1025\n",
      " 0.2462\n",
      " 0.2954\n",
      " 0.3090\n",
      " 0.3264\n",
      " 0.2689\n",
      " 0.3437\n",
      " 0.2767\n",
      "[torch.FloatTensor of size 11]\n",
      "\n",
      "Variable containing:\n",
      "  0.0000\n",
      "  1.1172\n",
      "  2.0013\n",
      "   â‹®    \n",
      "  0.2572\n",
      "  3.3491\n",
      "  0.0000\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "CPU times: user 32.4 ms, sys: 7.5 ms, total: 39.9 ms\n",
      "Wall time: 31.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-2.2970\n",
       "-2.3557\n",
       "-2.3056\n",
       "-2.2864\n",
       "-2.2960\n",
       "-2.3080\n",
       "-2.3067\n",
       "-2.2879\n",
       "-2.2928\n",
       "-2.2916\n",
       "[torch.FloatTensor of size 10]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time mem_net(Variable(dialog), Variable(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.06965174129353234, 0.4626865671641791)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(model, data, loss_func):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (dialog, images, target) in enumerate(data):\n",
    "        \n",
    "        pred = model(dialog, images).unsqueeze(0)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        loss = loss_func(pred, target)\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "    return total_loss / len(data)\n",
    "\n",
    "def predict(model, data):\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    \n",
    "    for i, (dialog, images, target) in enumerate(data):\n",
    "        \n",
    "        # For top 1:\n",
    "        pred = model(dialog, images).unsqueeze(0)\n",
    "        target = Variable(target)\n",
    "        \n",
    "        img, idx = torch.max(pred, 1)\n",
    "\n",
    "        if idx.data[0] == target.data[0]:\n",
    "            correct_top1 += 1\n",
    "        \n",
    "        # For top 5:\n",
    "        pred = pred.data.numpy().flatten()\n",
    "        top_5 = heapq.nlargest(5, range(len(pred)), pred.__getitem__)\n",
    "        if target.data[0] in top_5:\n",
    "            correct_top5 += 1\n",
    "    \n",
    "    return correct_top1 / len(data), correct_top5 / len(data)\n",
    "\n",
    "# validate(mem_net, valid_data, nn.NLLLoss())\n",
    "predict(mem_net, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_to_console(i, n_epochs, batch_size, batch_per_epoch, errors, start_time):\n",
    "    avgProcessingSpeed = (i*batch_size) / (time.time() - start_time)\n",
    "    percentOfEpc = (i / batch_per_epoch) * 100\n",
    "    print(\"{:.0f}s:\\t epoch: {}\\t batch:{} ({:.1f}%) \\t training error: {:.6f}\\t speed: {:.1f} dialogs/s\"\n",
    "          .format(time.time() - start_time, \n",
    "                  n_epochs, \n",
    "                  i, \n",
    "                  percentOfEpc, \n",
    "                  np.mean(errors[-100:]), \n",
    "                  avgProcessingSpeed))\n",
    "    \n",
    "def init_stats_log(label, training_portion, validation_portion, embeddings_dim, epochs, batch_count):\n",
    "    timestr = time.strftime(\"%m-%d-%H-%M\")\n",
    "    filename = \"{}-t_size_{}-v_size_{}-emb_{}-eps_{}-dt_{}-batch_{}.txt\".format(label,\n",
    "                                                                       training_portion,\n",
    "                                                                       validation_portion,\n",
    "                                                                       EMBEDDING_DIM,\n",
    "                                                                       epochs,\n",
    "                                                                       timestr,\n",
    "                                                                       batch_count)\n",
    "\n",
    "    target_path = ['Training_recordings', filename]\n",
    "    stats_log = open(os.path.join(*target_path), 'w')\n",
    "    stats_log.write(\"EPOCH|AVG_LOSS|TOT_LOSS|VAL_ERROR|CORRECT_TOP1|CORRECT_TOP5\\n\")\n",
    "    print(\"Logging enabled in:\", filename)\n",
    "    \n",
    "    return stats_log, filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 30\n",
    "numEpochs = 5\n",
    "learningRate = 1e-1\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "startTime = time.time()\n",
    "lastPrintTime = startTime\n",
    "\n",
    "continueFromEpc = 0\n",
    "continueFromI = 0\n",
    "sampleCount = len(dialog_data)\n",
    "batchCountPerEpc = int(sampleCount/batchSize)-1\n",
    "remainderCount = sampleCount - batchCountPerEpc * batchSize\n",
    "print(\"we have: {} dialogs, batch size of {} with {} as remainder to offset batches each epoch\".format(sampleCount, batchSize, remainderCount))\n",
    "offset = 0\n",
    "\n",
    "logging = False\n",
    "\n",
    "training_portion = len(dialog_data)\n",
    "validation_portion = len(valid_data)\n",
    "\n",
    "if logging == True:\n",
    "    stats_log, filename = init_stats_log(\"test_top1_top2\", \n",
    "                               training_portion,\n",
    "                               validation_portion,\n",
    "                               EMBEDDING_DIM,\n",
    "                               numEpochs,\n",
    "                               batchSize)\n",
    "\n",
    "else:\n",
    "    print(\"Logging disabled!\")\n",
    "    filename = \"\"\n",
    "\n",
    "for t in range(numEpochs):\n",
    "    lastPrintTime = time.time()\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    total_loss = 0\n",
    "    updates = 0\n",
    "    \n",
    "    if t == 0 and continueFromI > 0:\n",
    "        # continue where I crashed\n",
    "        print(\"continuing\")\n",
    "        model.load_state_dict(torch.load('maxent_{}epc_{}iter.pt'.format(continueFromEpc, continueFromI+1)))\n",
    "    \n",
    "    for i in range(continueFromI, batchCountPerEpc):\n",
    "        \n",
    "        # In case of RNN, clear hidden state\n",
    "        #model.hidden = steerNet.init_hidden(batchSize)\n",
    "        \n",
    "        batchBegin = offset + i * batchSize\n",
    "        batchEnd = batchBegin + batchSize\n",
    "        \n",
    "        batch = dialog_data[batchBegin:batchEnd]\n",
    "        inputs, targets = model.prepareBatch(batch)\n",
    "        \n",
    "        predictions = model(inputs, batchSize)\n",
    "        \n",
    "        loss = criterion(predictions.view(batchSize, -1), targets)\n",
    "        training_errors.append(loss.data[0])\n",
    "        total_loss += loss.data[0]\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if time.time()  - lastPrintTime > 10:\n",
    "            log_to_console(i, t, batchSize, batchCountPerEpc, training_errors, startTime)\n",
    "            lastPrintTime = time.time()\n",
    "    \n",
    "    avg_loss = total_loss / training_portion\n",
    "    top_1_score, top_5_score = predict(model, valid_data)\n",
    "    validation_error = validate(model, valid_data, criterion)\n",
    "    \n",
    "    if logging == True:\n",
    "        stats_log.write(\"{}|{}|{}|{}|{}|{}\\n\".format(epoch, avg_loss, total_loss, validation_error, top_1_score, top_5_score))\n",
    "            \n",
    "    epochsTrained += 1\n",
    "    offset = (offset + 1) % remainderCount\n",
    "    print(\"{:.1f}s:\\t Finished epoch. Calculating test error..\".format(time.time() - startTime))\n",
    "    print(\"{:.1f}s:\\t test error: {:.6f}\".format(time.time() - startTime, validation_error))\n",
    "    continueFromI = 0\n",
    "    fileName = \"maxent_{}batch_{}epc.pt\".format(batchSize, epochsTrained)\n",
    "    torch.save(model.state_dict(), fileName)\n",
    "    print(\"saved\\t\", fileName)\n",
    "\n",
    "if logging == True:\n",
    "    stats_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(filename=trained_stats_file):\n",
    "    \n",
    "    \n",
    "    # Read file and data\n",
    "    with open(\"Training_recordings/\" + filename, 'r') as f:\n",
    "        data = [x.strip() for x in f.readlines()] \n",
    "    \n",
    "    data = np.array([line.split(\"|\") for line in data[1:]]).T\n",
    "    \n",
    "    epochs, avg_loss, total_loss, val_error, correct_top_1, correct_top_5 = data\n",
    "    \n",
    "    epochs = np.array(epochs, dtype=np.int8)\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(epochs, np.array(avg_loss, dtype=np.float32), '.-')\n",
    "    plt.title('average loss, validation error and correct predictions')\n",
    "    plt.ylabel('Average\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(epochs, np.array(val_error, dtype=np.float32), '-')\n",
    "    plt.ylabel('Validation\\nLoss')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(epochs, np.array(correct_top_1, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 1')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    plt.subplot(4, 1, 4)\n",
    "    plt.plot(epochs, np.array(correct_top_5, dtype=np.int8), '-')\n",
    "    plt.ylabel('Correct\\ntop 5')\n",
    "    plt.xlabel('Epochs')\n",
    "\n",
    "    \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "draw_graph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
