{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is this a child or adult ? adult']\n",
      "[\"what color is horse ? brown, but it's black and white photo\"]\n",
      "['how many bikes there ? 3']\n",
      "['what color is the sink ? white']\n",
      "['is this a zoo ? yes']\n",
      "['is this a zoo ? yes']\n",
      "['is this a child or adult ? adult']\n",
      "['what color is the sink ? white']\n",
      "[\"what color is horse ? brown, but it's black and white photo\"]\n",
      "['how many bikes there ? 3']\n",
      "{'dialog': [('is', 'what', 'what', 'how'), ('this', 'color', 'color', 'many'), ('a', 'is', 'is', 'bikes'), ('zoo', 'the', 'horse', 'there'), ('?', 'sink', '?', '?'), ('yes', '?', 'brown,', '3'), ('how', 'white', 'but', 'what'), ('many', 'is', \"it's\", 'color'), ('giraffes', 'the', 'black', 'are'), ('are', 'light', 'and', 'bikes'), ('there', 'on', 'white', '?'), ('?', '?', 'photo', 'i'), ('1', 'yes', 'is', 'see'), ('how', 'any', 'this', 'green'), ('many', 'people', 'outdoors', 'red'), ('zebras', '?', '?', 'and'), ('?', 'no', 'yes', 'white'), ('1', 'how', 'do', 'are'), ('are', 'many', 'you', 'they'), ('people', 'dishes', 'see', 'parked'), ('there', 'are', 'any', 'on'), ('?', 'visible', 'horses', 'stock'), ('no', '?', '?', 'parking'), ('what', '0', 'yes,', '?'), ('did', 'any', '1', 'no'), ('the', 'windows', 'how', 'are'), ('giraffe', '?', 'about', 'there'), ('eat', '1', 'fences', 'any'), ('?', 'can', '?', 'people'), ('not', 'you', 'no', '?'), ('sure', 'see', 'do', '2'), ('it', 'food', 'you', 'what'), ('is', '?', 'see', 'are'), ('eating', 'no', 'any', 'their'), ('out', 'any', 'buildings', 'genders'), ('of', 'pots', '?', '?'), ('bin', '?', 'two,', \"can't\"), ('facing', 'no', '1', 'tell'), ('other', 'any', 'is', 'have'), ('way', 'utensils', 'brick,', 'on'), ('is', '?', '1', 'helmets'), ('it', 'no', 'is', 'what'), ('sunny', 'what', 'wood', 'color'), ('?', 'color', 'are', 'of'), ('yes', 'is', 'they', 'their'), ('how', 'the', 'single', 'helmets'), ('many', 'dish', 'story', '?'), ('trees', '?', '?', 'green'), ('are', 'no', 'yes', 'and'), ('there', 'dishes', 'do', 'white'), ('?', 'can', 'they', 'and'), ('6', 'you', 'look', 'other'), ('what', 'see', 'like', 'is'), ('colors', 'tile', 'homes', 'black'), ('is', '?', 'or', 'do'), ('the', 'yes', 'barns', 'they'), ('feeding', 'a', '?', 'pose'), ('bin', 'kitchen', 'they', 'to'), ('?', 'with', 'are', 'picture'), ('black', 'light', 'old', '?'), ('are', 'colored', 'stores', 'no'), ('there', 'wood', 'from', 'are'), ('stripes', 'cabinets,', 'time', 'they'), ('on', 'a', 'long', 'standing'), ('it', 'black', 'ago', 'by'), ('?', 'dishwasher', 'do', 'their'), ('no', 'and', 'they', 'bikes'), ('is', 'a', 'have', '?'), ('the', 'sink', 'signs', 'no'), ('wind', 'in', '?', 'what'), ('blowing', 'front', 'nol', 'are'), ('?', 'or', 'do', 'they'), ('not', 'a', 'you', 'doing'), ('sure', 'window', 'see', '?')], 'img_ids': [\n",
      " 3.5102e+04\n",
      " 3.0838e+05\n",
      " 4.3269e+05\n",
      " 4.3269e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.8620e+05\n",
      " 4.1933e+05\n",
      " 1.4514e+05\n",
      " 3.9703e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.2321e+05\n",
      " 5.4116e+05\n",
      " 3.2161e+05\n",
      " 5.2413e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.7943e+05\n",
      " 5.4891e+05\n",
      " 1.0885e+05\n",
      " 3.3034e+04\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 4.6150e+05\n",
      " 3.7846e+05\n",
      " 4.6889e+05\n",
      " 1.1542e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 2.5932e+05\n",
      " 4.2139e+05\n",
      " 2.9405e+04\n",
      " 3.3036e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 4.1157e+05\n",
      " 3.6141e+05\n",
      " 3.7695e+05\n",
      " 3.6001e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 1.7648e+05\n",
      " 5.5600e+05\n",
      " 2.7069e+05\n",
      " 3.2445e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.3224e+05\n",
      " 2.1494e+05\n",
      " 5.7503e+05\n",
      " 1.9755e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 5.5398e+05\n",
      " 3.3631e+05\n",
      " 4.8016e+05\n",
      " 2.8714e+05\n",
      "[torch.LongTensor of size 4]\n",
      "], 'target': \n",
      " 3.3224e+05\n",
      " 3.7846e+05\n",
      " 5.7503e+05\n",
      " 2.8714e+05\n",
      "[torch.LongTensor of size 4]\n",
      "}\n",
      "{'dialog': [('is',), ('this',), ('a',), ('child',), ('or',), ('adult',), ('?',), ('adult',), ('male',), ('or',), ('female',), ('?',), ('male',), ('are',), ('they',), ('inside',), ('or',), ('outside',), ('?',), ('inside',), ('are',), ('they',), ('laying',), ('on',), ('the',), ('floor',), ('?',), ('yes,',), ('but',), ('there',), ('is',), ('a',), ('blanket',), ('in',), ('between',), ('them',), ('and',), ('the',), ('floor',), ('is',), ('the',), ('floor',), ('carpeted',), ('or',), ('wooden',), ('?',), ('it',), ('is',), ('tile',), ('what',), ('color',), ('is',), ('the',), ('blanket',), ('?',), ('red',), ('and',), ('white',), ('what',), ('color',), ('is',), ('the',), ('tile',), ('?',), ('orange',), ('red',), ('what',), ('breed',), ('is',), ('the',), ('dog',), ('?',), ('boxer',), ('does',), ('the',), ('dog',), ('look',), ('healthy',), ('and',), ('happy',), ('?',), ('yes',), ('what',), ('color',), ('is',), ('the',), ('dog',), ('?',), ('tan',), ('a',), ('person',), ('that',), ('is',), ('laying',), ('next',), ('to',), ('a',), ('dog',)], 'img_ids': [\n",
      " 1.6339e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.7847e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 2.1648e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 1.1468e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 4.5582e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 5.3419e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 56961\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.2874e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 84776\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 10991\n",
      "[torch.LongTensor of size 1]\n",
      "], 'target': \n",
      " 3.7847e+05\n",
      "[torch.LongTensor of size 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, json_data, transform=None):\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.json_data.iloc[idx]\n",
    "        print(item.dialog[0])\n",
    "\n",
    "        # Flatten dialog and add caption into 1d array\n",
    "        dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "        dialog.extend(item.caption.split(' '))\n",
    "        #words = np.array(dialog)\n",
    "\n",
    "        img_ids = np.array(item.img_list)\n",
    "        target = np.array([item.target, item.target_img_id])\n",
    "\n",
    "        return {'dialog':dialog, 'img_ids':item.img_list, 'target':item.target_img_id}\n",
    "\n",
    "def show_batch(sample_batched):\n",
    "    print(sample_batched)\n",
    "\n",
    "\n",
    "def createEmbeddings (words, threshold):\n",
    "    w2i = defaultdict(lambda: len(w2i))\n",
    "    i2w = dict()\n",
    "    wordCounts = Counter()\n",
    "\n",
    "    # count all the words in lower case\n",
    "    for word in words:\n",
    "        wordCounts[word.lower()] += 1\n",
    "\n",
    "    # index all words that occured at least n times\n",
    "    for word, count in wordCounts.most_common():\n",
    "        if count >= threshold:\n",
    "            i2w[w2i[word]] = word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return w2i, i2w\n",
    "\n",
    "# done: collect all the words from dialogs and \n",
    "# captions and use them to create embedding map\n",
    "def getWords(dataset):\n",
    "    words = [dataset[i]['dialog'] for i in range(len(dataset))]\n",
    "    return list(chain.from_iterable(words))\n",
    "\n",
    "    \n",
    "dd = DialogDataset(os.path.join(*SAMPLE_EASY))\n",
    "\n",
    "words = getWords(dd)\n",
    "w2i, i2w = createEmbeddings(words, 3)\n",
    "\n",
    "loader = DataLoader(dd, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for batch_num, sample in enumerate(loader):\n",
    "    show_batch(sample)\n",
    "    if batch_num == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Embedding neural net class:\n",
      "Variable containing:\n",
      "-1.0952 -1.0703  0.6404  1.6199  0.5258\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2 or 4 dimensions (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-00c9aea6c077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-149-00c9aea6c077>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             loss = loss_func(log_probs, autograd.Variable(\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             ))\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/workenv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/workenv/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 482\u001b[0;31m                                self.ignore_index)\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/workenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    744\u001b[0m                 \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0maveraged\u001b[0m \u001b[0mover\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mignored\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m     \"\"\"\n\u001b[0;32m--> 746\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/workenv/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index)\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected 2 or 4 dimensions (got {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2 or 4 dimensions (got 1)"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#### Testing \n",
    "print(\"Testing Embedding neural net class:\")\n",
    "embeds = nn.Embedding(len(w2i), 5)\n",
    "lookup_tensor = torch.LongTensor([w2i['bikes']])\n",
    "result = embeds(autograd.Variable(lookup_tensor))\n",
    "print(result)\n",
    "\n",
    "\n",
    "context_size = 2\n",
    "data = []\n",
    "#Find two words before, and two words after given word.\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1],\n",
    "               words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self, context_size=2, embedding_size=100, vocab_size=None):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.linear1 = nn.Linear(embedding_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        lookup_embeds = self.embeddings(inputs)\n",
    "        embeds = lookup_embeds.sum(dim=0)\n",
    "        out = self.linear1(embeds)\n",
    "        out = F.log_softmax(out)\n",
    "        return out\n",
    "\n",
    "def make_context_vector(context, w2i):\n",
    "    idxs = [w2i[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "\n",
    "def run():\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    net = CBOW(context_size, embedding_size=5, vocab_size=len(w2i))\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(100):\n",
    "        total_loss = 0\n",
    "        for context, target in data:\n",
    "            context_var = make_context_vector(context, w2i)\n",
    "            net.zero_grad()\n",
    "            log_probs = net(context_var)\n",
    "\n",
    "            loss = loss_func(log_probs, autograd.Variable(\n",
    "                torch.LongTensor([w2i[target]])\n",
    "            ))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.data\n",
    "        print(total_loss)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
