{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, json_data, image_features, img2feat, transform=None):\n",
    "        \n",
    "        with open(img2feat, 'r') as f:\n",
    "            self.img2feat = json.load(f)['IR_imgid2id']\n",
    "            \n",
    "        self.img_features = np.asarray(h5py.File(image_features, 'r')['img_features'])\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = [datapoint['dialog'] for datapoint in self]\n",
    "        return list(chain.from_iterable(words))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.json_data.iloc[idx]\n",
    "\n",
    "        # Flatten dialog and add caption into 1d array\n",
    "        dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "        dialog.extend(item.caption.split(' '))\n",
    "\n",
    "        img_ids = np.array(item.img_list)\n",
    "        img_features = [self.img_features[idx] for idx in map(lambda x: self.img2feat[str(x)], img_ids)]\n",
    "        \n",
    "        return {\n",
    "            'dialog' : dialog, \n",
    "            'img_ids': item.img_list, \n",
    "            'img_features': img_features, \n",
    "            'target_idx' : item.target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "def make_context_vector(context, w2i):\n",
    "    idxs = [w2i[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "IMG_FEATURES = ['Data', 'Features', 'IR_image_features.h5']\n",
    "INDEX_MAP = ['Data', 'Features', 'IR_img_features2id.json']\n",
    "\n",
    "EMBEDDING_DIM = 5\n",
    "CONTEXT_SIZE = 2\n",
    "FREQ_THRESHOLD = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "\n",
    "w2i = {word : i for i, word in enumerate(dialog_data.vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text feature: \n",
      "\n",
      " Variable containing:\n",
      " 0.2252\n",
      " 0.3313\n",
      " 0.0773\n",
      " 0.2268\n",
      " 0.2515\n",
      " 0.0630\n",
      " 0.8003\n",
      " 0.9296\n",
      " 0.9980\n",
      " 0.9917\n",
      " 0.5473\n",
      " 0.0008\n",
      " 0.0174\n",
      " 0.0071\n",
      " 0.9171\n",
      " 0.8011\n",
      " 0.5299\n",
      " 0.0669\n",
      " 0.1455\n",
      " 0.9086\n",
      " 0.0833\n",
      " 0.4478\n",
      " 0.1292\n",
      " 0.2738\n",
      " 0.7710\n",
      " 0.0682\n",
      " 0.0032\n",
      " 0.9940\n",
      " 0.0495\n",
      " 0.9854\n",
      " 0.8881\n",
      " 0.0815\n",
      " 0.2378\n",
      " 0.0839\n",
      " 0.6673\n",
      " 0.0253\n",
      " 0.1221\n",
      " 0.9273\n",
      " 0.9107\n",
      " 0.0012\n",
      " 0.2854\n",
      " 0.9495\n",
      " 0.3647\n",
      " 0.9009\n",
      " 0.9300\n",
      " 0.4066\n",
      " 0.8211\n",
      " 0.9407\n",
      " 0.8537\n",
      " 0.8849\n",
      " 0.0128\n",
      " 0.3720\n",
      " 0.7848\n",
      " 0.1335\n",
      " 0.8912\n",
      " 0.7791\n",
      " 0.0327\n",
      " 0.0360\n",
      " 0.0013\n",
      " 0.9996\n",
      " 0.9251\n",
      " 0.1482\n",
      " 0.3513\n",
      " 0.0427\n",
      " 0.5312\n",
      " 0.8600\n",
      " 0.7299\n",
      " 0.0025\n",
      " 0.0003\n",
      " 0.0773\n",
      " 0.8056\n",
      " 0.9514\n",
      " 0.0523\n",
      " 0.8842\n",
      " 0.0043\n",
      " 0.0091\n",
      " 0.5931\n",
      " 0.0021\n",
      " 0.0291\n",
      " 0.0646\n",
      " 0.2122\n",
      " 0.6559\n",
      " 0.2913\n",
      " 0.9679\n",
      " 0.9846\n",
      " 0.9062\n",
      " 0.9314\n",
      " 0.7849\n",
      " 0.8816\n",
      " 0.1949\n",
      " 0.3745\n",
      " 0.9639\n",
      " 0.3350\n",
      " 0.1424\n",
      " 0.9958\n",
      " 0.0194\n",
      " 0.5908\n",
      " 0.9524\n",
      " 0.5718\n",
      " 0.9399\n",
      " 0.9192\n",
      " 0.8346\n",
      " 0.9544\n",
      " 0.8826\n",
      " 0.0204\n",
      " 0.5674\n",
      " 0.9788\n",
      " 0.5429\n",
      " 0.0864\n",
      " 0.0099\n",
      " 0.8657\n",
      " 0.0230\n",
      " 0.0004\n",
      " 0.3159\n",
      " 0.9708\n",
      " 0.9996\n",
      " 0.3085\n",
      " 0.0604\n",
      " 0.9926\n",
      " 0.4004\n",
      " 0.5817\n",
      " 0.2569\n",
      " 0.0249\n",
      " 0.6365\n",
      " 0.3795\n",
      " 0.4066\n",
      " 0.9060\n",
      " 0.9983\n",
      " 0.0583\n",
      " 0.3086\n",
      " 0.4353\n",
      " 0.9093\n",
      " 0.3540\n",
      " 0.2650\n",
      " 0.5748\n",
      " 0.0146\n",
      " 0.1072\n",
      " 0.1976\n",
      " 0.4174\n",
      " 0.9996\n",
      " 0.0125\n",
      " 0.0486\n",
      " 0.9930\n",
      " 0.1836\n",
      " 0.8844\n",
      " 0.0324\n",
      " 0.3278\n",
      " 0.8370\n",
      " 0.6427\n",
      " 0.6201\n",
      " 0.9860\n",
      " 0.9585\n",
      " 0.3147\n",
      " 0.3272\n",
      " 0.7154\n",
      " 0.0509\n",
      " 0.0968\n",
      " 0.0209\n",
      " 0.9033\n",
      " 0.2376\n",
      " 0.9995\n",
      " 0.0824\n",
      " 0.0462\n",
      "[torch.FloatTensor of size 163]\n",
      "\n",
      "Sample image feature: \n",
      "\n",
      " Variable containing:\n",
      " 0.0732\n",
      " 0.3372\n",
      " 0.1992\n",
      "   ⋮   \n",
      " 0.5809\n",
      " 0.1955\n",
      " 0.1294\n",
      "[torch.FloatTensor of size 2048]\n",
      "\n",
      "Variable containing:\n",
      " 0.9306\n",
      " 0.6097\n",
      " 0.8695\n",
      "   ⋮   \n",
      " 0.5809\n",
      " 0.1955\n",
      " 0.1294\n",
      "[torch.FloatTensor of size 2211]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cbow_model = CBOW(len(w2i), EMBEDDING_DIM)\n",
    "inp = make_context_vector(dialog_data[0]['dialog'], w2i)\n",
    "text_feat = torch.squeeze(cbow_model(inp), 0)\n",
    "print(\"Sample text feature: \\n\\n\", out)\n",
    "\n",
    "sample_img_feat = Variable(torch.FloatTensor(dialog_data[0]['img_features'][0]))\n",
    "print(\"Sample image feature: \\n\\n\", sample_img_feat)\n",
    "\n",
    "concat_features = torch.cat((out, sample_img_feat), 0)\n",
    "print(concat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
