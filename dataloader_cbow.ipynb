{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, json_data, image_features, img2feat, transform=None):\n",
    "        \n",
    "        with open(img2feat, 'r') as f:\n",
    "            self.img2feat = json.load(f)['IR_imgid2id']\n",
    "            \n",
    "        self.img_features = np.asarray(h5py.File(image_features, 'r')['img_features'])\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        self.w2i = {word : i for i, word in enumerate(self.vocab)}\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = []\n",
    "        for idx in range(len(self)):\n",
    "            item = self.json_data.iloc[idx]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "\n",
    "            words.append(dialog)\n",
    "            \n",
    "        return list(chain.from_iterable(words))\n",
    "    \n",
    "    def make_context_vector(self, context):\n",
    "        idxs = [self.w2i[w] for w in context]\n",
    "        tensor = torch.LongTensor(idxs)\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            #Get the start, stop, and step from the slice\n",
    "            return [self[ii] for ii in range(*key.indices(len(self)))]\n",
    "        elif isinstance(key, int):\n",
    "            if key < 0 : #Handle negative indices\n",
    "                key += len( self )\n",
    "            if key < 0 or key >= len(self) :\n",
    "                raise IndexError(\"The index ({}) is out of range.\".format(key))\n",
    "            \n",
    "            item = self.json_data.iloc[key]\n",
    "\n",
    "            # Flatten dialog and add caption into 1d array\n",
    "            dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "            dialog.extend(item.caption.split(' '))\n",
    "            dialog = self.make_context_vector(dialog)\n",
    "\n",
    "            img_ids = np.array(item.img_list)\n",
    "            img_features = [self.img_features[idx] for idx in map(lambda x: self.img2feat[str(x)], img_ids)]\n",
    "            img_features = np.array(img_features)\n",
    "            img_features = torch.FloatTensor(img_features)\n",
    "\n",
    "            target = item.target\n",
    "            target = torch.LongTensor(np.array([target]))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                dialog, img_features, target = dialog.cuda(), img_features.cuda(), target.cuda()\n",
    "                \n",
    "            return dialog, img_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "VALID_EASY = ['Data', 'Easy', 'IR_val_easy.json']\n",
    "IMG_FEATURES = ['Data', 'Features', 'IR_image_features.h5']\n",
    "INDEX_MAP = ['Data', 'Features', 'IR_img_features2id.json']\n",
    "\n",
    "IMG_SIZE = 2048\n",
    "EMBEDDING_DIM = 5\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "dialog_data = DialogDataset(os.path.join(*TRAIN_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "valid_data = DialogDataset(os.path.join(*VALID_EASY), os.path.join(*IMG_FEATURES), os.path.join(*INDEX_MAP))\n",
    "\n",
    "vocab_size = len(dialog_data.vocab)\n",
    "\n",
    "print(len(dialog_data[0:3])) # can now slice this bitch up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxEnt(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, text_module, vocab_size, img_size):\n",
    "        super(MaxEnt, self).__init__()\n",
    "\n",
    "        self.text_module = text_module\n",
    "        self.linear = nn.Linear(vocab_size + img_size, 1)\n",
    "        self.softmax = nn.LogSoftmax()\n",
    "        \n",
    "    def prepare (self, dialog, imgFeatures):\n",
    "        text_features = self.text_module(Variable(dialog))\n",
    "        text_features = text_features.expand(imgFeatures.size(0), text_features.size(1))\n",
    "        concat = torch.cat((imgFeatures, text_features.data), 1)\n",
    "        return concat\n",
    "    \n",
    "    def prepareBatch (self, batch):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        for dialog, imgFeatures, target in batch:\n",
    "            inputs.append(self.prepare(dialog, imgFeatures))\n",
    "            targets.append(target)\n",
    "        inputs = torch.cat(inputs)\n",
    "        targets = torch.cat(targets)\n",
    "        return Variable(inputs), Variable(targets)\n",
    "        \n",
    "    def forward(self, inp, batchSize=1):\n",
    "        scores = self.linear(inp)\n",
    "        scores = self.softmax(scores.transpose(0, 1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ya ya\n",
      "cuda ready bitches\n"
     ]
    }
   ],
   "source": [
    "cbow_model = CBOW(vocab_size, EMBEDDING_DIM)\n",
    "model = MaxEnt(cbow_model, vocab_size, IMG_SIZE)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ya ya\")\n",
    "    cbow_model = cbow_model.cuda()\n",
    "    model = model.cuda()\n",
    "    print(\"cuda ready bitches\")\n",
    "else:\n",
    "    print(\"no no\")\n",
    "    \n",
    "training_errors = []\n",
    "validation_errors = []\n",
    "epochsTrained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.300809121131897"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate(model, data, loss_func):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, (dialog, imgFeatures, target) in enumerate(data):\n",
    "        inputs = model.prepare(dialog, imgFeatures)\n",
    "        \n",
    "        inputs, target = Variable(inputs), Variable(target)\n",
    "        \n",
    "        pred = model(inputs)\n",
    "        \n",
    "        loss = loss_func(pred, target)\n",
    "        total_loss += loss.data[0]\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "def predict(model, data):\n",
    "    correct = 0\n",
    "    \n",
    "    for i, (inp, target) in enumerate(data):\n",
    "        pred = model(inp)\n",
    "        img, idx = torch.max(pred, 1)\n",
    "        if idx.data[0] == target:\n",
    "            correct += 1\n",
    "        \n",
    "        if i == 20:\n",
    "            break\n",
    "    \n",
    "    return correct\n",
    "validate(model, valid_data[:100], nn.NLLLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have: 40000 dialogs, batch size of 1 with 1 as remainder to offset batches each epoch\n",
      "10s:\t epoch: 3\t batch:3135 (7.8%) \t training error: 2.242770\t speed: 313.5 dialogs/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-682fe4d4deef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# for epoch in range(1, EPOCHS + 1):\n",
    "#     total_loss = 0\n",
    "#     for i, (inp, target) in enumerate(dialog_data):\n",
    "        \n",
    "#         pred = max_ent(inp)\n",
    "        \n",
    "#         target = Variable(torch.LongTensor(np.array([target])))\n",
    "\n",
    "#         loss = loss_func(pred, target)\n",
    "#         total_loss += loss.data[0]\n",
    "            \n",
    "#         max_ent.zero_grad()\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         if i == 500:\n",
    "#             break\n",
    "    \n",
    "#     total_loss = total_loss / 500\n",
    "#     print(\"Epoch {}: {}\".format(epoch, total_loss))\n",
    "#     print(\"Predicted {}/20 samples correctly\".format(predict(max_ent, valid_data)))\n",
    "    \n",
    "#     val = validate(max_ent, valid_data, loss_func)\n",
    "#     validation_errors.append(val.data[0])\n",
    "        \n",
    "import time\n",
    "\n",
    "batchSize = 30\n",
    "numEpochs = 3\n",
    "learningRate = 1e-4\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "startTime = time.time()\n",
    "lastPrintTime = startTime\n",
    "\n",
    "continueFromEpc = 0\n",
    "continueFromI = 0\n",
    "sampleCount = len(dialog_data)\n",
    "batchCountPerEpc = int(sampleCount/batchSize)-1\n",
    "remainderCount = sampleCount - batchCountPerEpc * batchSize\n",
    "print(\"we have: {} dialogs, batch size of {} with {} as remainder to offset batches each epoch\".format(sampleCount, batchSize, remainderCount))\n",
    "offset = 0\n",
    "\n",
    "for t in range(numEpochs):\n",
    "    lastPrintTime = time.time()\n",
    "    epochStartTime = time.time()\n",
    "    \n",
    "    if t == 0 and continueFromI > 0:\n",
    "        # continue where I crashed\n",
    "        print(\"continuing\")\n",
    "        model.load_state_dict(torch.load('maxent_{}epc_{}iter.pt'.format(continueFromEpc, continueFromI+1)))\n",
    "    \n",
    "    for i in range(continueFromI, batchCountPerEpc):\n",
    "        # In case of RNN, clear hidden state\n",
    "        #model.hidden = steerNet.init_hidden(batchSize)\n",
    "        \n",
    "        batchBegin = offset + i * batchSize\n",
    "        batchEnd = batchBegin + batchSize\n",
    "        \n",
    "        batch = dialog_data[batchBegin:batchEnd]\n",
    "        inputs, targets = model.prepareBatch(batch)\n",
    "        \n",
    "        predictions = model(inputs, batchSize)\n",
    "        \n",
    "        loss = criterion(predictions.view(batchSize, -1), targets)\n",
    "        training_errors.append(loss.data[0])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # report and save progress sometimes\n",
    "        if (time.time() - lastPrintTime > 10):\n",
    "            lastPrintTime = time.time()\n",
    "            avgProcessingSpeed = (i*batchSize) / (time.time() - epochStartTime)\n",
    "            percentOfEpc = (i / batchCountPerEpc) * 100\n",
    "            print(\"{:.0f}s:\\t epoch: {}\\t batch:{} ({:.1f}%) \\t training error: {:.6f}\\t speed: {:.1f} dialogs/s\".format(time.time() - startTime, epochsTrained, i, percentOfEpc, np.mean(training_errors[-100:]), avgProcessingSpeed))\n",
    "#             if (i % 10000 == 0):\n",
    "#                 torch.save(model.state_dict(),\"maxent_{}epc_{}iter.pt\".format(t, i+1))\n",
    "#                 print(\"saved at {}\".format(i))\n",
    "            \n",
    "    epochsTrained += 1\n",
    "    offset = (offset + 1) % remainderCount\n",
    "    print(\"{:.1f}s:\\t Finished epoch. Calculating test error..\".format(time.time() - startTime))\n",
    "    testError = validate(model, valid_data, nn.NLLLoss())\n",
    "    print(\"{:.1f}s:\\t test error: {:.6f}\".format(time.time() - startTime, testError))\n",
    "    continueFromI = 0\n",
    "    fileName = \"maxent_{}batch_{}epc.pt\".format(batchSize, epochsTrained)\n",
    "    torch.save(model.state_dict(), fileName)\n",
    "    print(\"saved\\t\", fileName)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
