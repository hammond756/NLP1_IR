{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is this a child or adult ? adult']\n",
      "[\"what color is horse ? brown, but it's black and white photo\"]\n",
      "['how many bikes there ? 3']\n",
      "['what color is the sink ? white']\n",
      "['is this a zoo ? yes']\n",
      "['is this a zoo ? yes']\n",
      "['is this a child or adult ? adult']\n",
      "[\"what color is horse ? brown, but it's black and white photo\"]\n",
      "['what color is the sink ? white']\n",
      "['how many bikes there ? 3']\n",
      "{'dialog': [('is', 'is', 'what', 'what'), ('this', 'this', 'color', 'color'), ('a', 'a', 'is', 'is'), ('zoo', 'child', 'horse', 'the'), ('?', 'or', '?', 'sink'), ('yes', 'adult', 'brown,', '?'), ('how', '?', 'but', 'white'), ('many', 'adult', \"it's\", 'is'), ('giraffes', 'male', 'black', 'the'), ('are', 'or', 'and', 'light'), ('there', 'female', 'white', 'on'), ('?', '?', 'photo', '?'), ('1', 'male', 'is', 'yes'), ('how', 'are', 'this', 'any'), ('many', 'they', 'outdoors', 'people'), ('zebras', 'inside', '?', '?'), ('?', 'or', 'yes', 'no'), ('1', 'outside', 'do', 'how'), ('are', '?', 'you', 'many'), ('people', 'inside', 'see', 'dishes'), ('there', 'are', 'any', 'are'), ('?', 'they', 'horses', 'visible'), ('no', 'laying', '?', '?'), ('what', 'on', 'yes,', '0'), ('did', 'the', '1', 'any'), ('the', 'floor', 'how', 'windows'), ('giraffe', '?', 'about', '?'), ('eat', 'yes,', 'fences', '1'), ('?', 'but', '?', 'can'), ('not', 'there', 'no', 'you'), ('sure', 'is', 'do', 'see'), ('it', 'a', 'you', 'food'), ('is', 'blanket', 'see', '?'), ('eating', 'in', 'any', 'no'), ('out', 'between', 'buildings', 'any'), ('of', 'them', '?', 'pots'), ('bin', 'and', 'two,', '?'), ('facing', 'the', '1', 'no'), ('other', 'floor', 'is', 'any'), ('way', 'is', 'brick,', 'utensils'), ('is', 'the', '1', '?'), ('it', 'floor', 'is', 'no'), ('sunny', 'carpeted', 'wood', 'what'), ('?', 'or', 'are', 'color'), ('yes', 'wooden', 'they', 'is'), ('how', '?', 'single', 'the'), ('many', 'it', 'story', 'dish'), ('trees', 'is', '?', '?'), ('are', 'tile', 'yes', 'no'), ('there', 'what', 'do', 'dishes'), ('?', 'color', 'they', 'can'), ('6', 'is', 'look', 'you'), ('what', 'the', 'like', 'see'), ('colors', 'blanket', 'homes', 'tile'), ('is', '?', 'or', '?'), ('the', 'red', 'barns', 'yes'), ('feeding', 'and', '?', 'a'), ('bin', 'white', 'they', 'kitchen'), ('?', 'what', 'are', 'with'), ('black', 'color', 'old', 'light'), ('are', 'is', 'stores', 'colored'), ('there', 'the', 'from', 'wood'), ('stripes', 'tile', 'time', 'cabinets,'), ('on', '?', 'long', 'a'), ('it', 'orange', 'ago', 'black'), ('?', 'red', 'do', 'dishwasher'), ('no', 'what', 'they', 'and'), ('is', 'breed', 'have', 'a'), ('the', 'is', 'signs', 'sink'), ('wind', 'the', '?', 'in'), ('blowing', 'dog', 'nol', 'front'), ('?', '?', 'do', 'or'), ('not', 'boxer', 'you', 'a'), ('sure', 'does', 'see', 'window')], 'img_ids': [\n",
      " 3.5102e+04\n",
      " 1.6339e+05\n",
      " 4.3269e+05\n",
      " 3.0838e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.8620e+05\n",
      " 3.7847e+05\n",
      " 1.4514e+05\n",
      " 4.1933e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.2321e+05\n",
      " 2.1648e+05\n",
      " 3.2161e+05\n",
      " 5.4116e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.7943e+05\n",
      " 1.1468e+05\n",
      " 1.0885e+05\n",
      " 5.4891e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 4.6150e+05\n",
      " 4.5582e+05\n",
      " 4.6889e+05\n",
      " 3.7846e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 2.5932e+05\n",
      " 5.3419e+05\n",
      " 2.9405e+04\n",
      " 4.2139e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 4.1157e+05\n",
      " 5.6961e+04\n",
      " 3.7695e+05\n",
      " 3.6141e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 1.7648e+05\n",
      " 3.2874e+05\n",
      " 2.7069e+05\n",
      " 5.5600e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 3.3224e+05\n",
      " 8.4776e+04\n",
      " 5.7503e+05\n",
      " 2.1494e+05\n",
      "[torch.LongTensor of size 4]\n",
      ", \n",
      " 5.5398e+05\n",
      " 1.0991e+04\n",
      " 4.8016e+05\n",
      " 3.3631e+05\n",
      "[torch.LongTensor of size 4]\n",
      "], 'target': \n",
      " 3.3224e+05\n",
      " 3.7847e+05\n",
      " 5.7503e+05\n",
      " 3.7846e+05\n",
      "[torch.LongTensor of size 4]\n",
      "}\n",
      "{'dialog': [('how',), ('many',), ('bikes',), ('there',), ('?',), ('3',), ('what',), ('color',), ('are',), ('bikes',), ('?',), ('i',), ('see',), ('green',), ('red',), ('and',), ('white',), ('are',), ('they',), ('parked',), ('on',), ('stock',), ('parking',), ('?',), ('no',), ('are',), ('there',), ('any',), ('people',), ('?',), ('2',), ('what',), ('are',), ('their',), ('genders',), ('?',), (\"can't\",), ('tell',), ('have',), ('on',), ('helmets',), ('what',), ('color',), ('of',), ('their',), ('helmets',), ('?',), ('green',), ('and',), ('white',), ('and',), ('other',), ('is',), ('black',), ('do',), ('they',), ('pose',), ('to',), ('picture',), ('?',), ('no',), ('are',), ('they',), ('standing',), ('by',), ('their',), ('bikes',), ('?',), ('no',), ('what',), ('are',), ('they',), ('doing',), ('?',), ('sitting',), ('on',), ('bikes',), ('white',), ('time',), ('of',), ('day',), ('is',), ('it',), ('?',), ('looks',), ('like',), ('afternoon',), ('a',), ('couple',), ('of',), ('people',), ('and',), ('some',), ('motor',), ('bikes',)], 'img_ids': [\n",
      " 4.3269e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.9703e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 5.2413e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 33034\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 1.1542e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.3036e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.6001e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 3.2445e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 1.9755e+05\n",
      "[torch.LongTensor of size 1]\n",
      ", \n",
      " 2.8714e+05\n",
      "[torch.LongTensor of size 1]\n",
      "], 'target': \n",
      " 2.8714e+05\n",
      "[torch.LongTensor of size 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, json_data, transform=None):\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.json_data.iloc[idx]\n",
    "        print(item.dialog[0])\n",
    "\n",
    "        # Flatten dialog and add caption into 1d array\n",
    "        dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "        dialog.extend(item.caption.split(' '))\n",
    "        #words = np.array(dialog)\n",
    "\n",
    "        img_ids = np.array(item.img_list)\n",
    "        target = np.array([item.target, item.target_img_id])\n",
    "\n",
    "        return {'dialog':dialog, 'img_ids':item.img_list, 'target':item.target_img_id}\n",
    "\n",
    "def show_batch(sample_batched):\n",
    "    print(sample_batched)\n",
    "\n",
    "\n",
    "def createEmbeddings (words, threshold):\n",
    "    w2i = defaultdict(lambda: len(w2i))\n",
    "    i2w = dict()\n",
    "    wordCounts = Counter()\n",
    "\n",
    "    # count all the words in lower case\n",
    "    for word in words:\n",
    "        wordCounts[word.lower()] += 1\n",
    "\n",
    "    # index all words that occured at least n times\n",
    "    for word, count in wordCounts.most_common():\n",
    "        if count >= threshold:\n",
    "            i2w[w2i[word]] = word\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return w2i, i2w\n",
    "\n",
    "# done: collect all the words from dialogs and \n",
    "# captions and use them to create embedding map\n",
    "def getWords(dataset):\n",
    "    words = [dataset[i]['dialog'] for i in range(len(dataset))]\n",
    "    return list(chain.from_iterable(words))\n",
    "\n",
    "    \n",
    "dd = DialogDataset(os.path.join(*SAMPLE_EASY))\n",
    "\n",
    "words = getWords(dd)\n",
    "w2i, i2w = createEmbeddings(words, 3)\n",
    "\n",
    "loader = DataLoader(dd, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for batch_num, sample in enumerate(loader):\n",
    "    show_batch(sample)\n",
    "    if batch_num == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Embedding neural net class:\n",
      "Variable containing:\n",
      "-1.0952 -1.0703  0.6404  1.6199  0.5258\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "Found context sample:  Variable containing:\n",
      "  0\n",
      " 38\n",
      "  7\n",
      " 39\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "Lengths of w2i and i2w:\n",
      "40\n",
      "38\n",
      "163\n",
      "Prediction: 141\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "#### Testing \n",
    "print(\"Testing Embedding neural net class:\")\n",
    "embeds = nn.Embedding(len(w2i), 5)\n",
    "lookup_tensor = torch.LongTensor([w2i['bikes']])\n",
    "result = embeds(autograd.Variable(lookup_tensor))\n",
    "print(result)\n",
    "\n",
    "\n",
    "context_size = 2\n",
    "data = []\n",
    "#Find two words before, and two words after given word.\n",
    "for i in range(2, len(words) - 2):\n",
    "    context = [words[i - 2], words[i - 1],\n",
    "               words[i + 1], words[i + 2]]\n",
    "    target = words[i]\n",
    "    data.append((context, target))\n",
    "\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = Variable(torch.LongTensor([w2i[word]]))\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "def make_context_vector(context, w2i):\n",
    "    idxs = [w2i[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "def train_model():\n",
    "    model = CBOW(vocab_size, EMDEDDING_DIM)\n",
    "\n",
    "    loss_function = nn.NLLLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        total_loss = 0\n",
    "        print(epoch)\n",
    "        count = 0\n",
    "        for context, target in data:\n",
    "            context_vector = make_context_vector(context, w2i)\n",
    "            model.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                log_probs = model(context_vector)\n",
    "            except:\n",
    "                print(\"Iteration:\", count, \"Target word:\", target, \"\\nContext:\", context_vector)\n",
    "            \n",
    "            loss = loss_function(log_probs, Variable(torch.LongTensor([w2i[target]])))\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data\n",
    "            count += 1\n",
    "\n",
    "# Uncomment to train:\n",
    "# train()\n",
    "\n",
    "sample = make_context_vector(data[11][0], w2i)\n",
    "print(\"Found context sample: \", sample)\n",
    "a = model(sample).data.numpy()\n",
    "\n",
    "print(\"Lengths of w2i and i2w:\")\n",
    "print(len(w2i))\n",
    "print(len(i2w))\n",
    "# Something is wrong with i2w, it's not of the same length as w2i!\n",
    "print(\"Prediction:\", np.argmax(a[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
