{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import sys\n",
    "import pprint\n",
    "from collections import Counter,defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, json_data, transform=None):\n",
    "        self.json_data = pd.read_json(json_data, orient='index')\n",
    "        self.corpus = self.get_words()\n",
    "        self.vocab = list(set(self.corpus))\n",
    "        \n",
    "    # collect all the words from dialogs and \n",
    "    # captions and use them to create embedding map\n",
    "    def get_words(self):\n",
    "        words = [datapoint['dialog'] for datapoint in self]\n",
    "        return list(chain.from_iterable(words))\n",
    "    \n",
    "#     def filter_vocabulary(self):\n",
    "#         common_words = [word for word, count in Counter(self.corpus).most_common()]\n",
    "#         return common_words[:50]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.json_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.json_data.iloc[idx]\n",
    "\n",
    "        # Flatten dialog and add caption into 1d array\n",
    "        dialog = [word for line in item.dialog for word in line[0].split()]\n",
    "        dialog.extend(item.caption.split(' '))\n",
    "        #words = np.array(dialog)\n",
    "\n",
    "        img_ids = np.array(item.img_list)\n",
    "        target = np.array([item.target, item.target_img_id])\n",
    "\n",
    "        return {'dialog':dialog, 'img_ids':item.img_list, 'target':item.target_img_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Find n words before, and n words after given word\n",
    "# and add them to the dataset\n",
    "def scan_context(raw_text, n):\n",
    "    data = []\n",
    "    for i in range(n, len(raw_text) - n):\n",
    "        context = raw_text[i-n:i] + raw_text[i+1:i+n+1]\n",
    "        target = raw_text[i]\n",
    "        data.append((context, target))\n",
    "    return data\n",
    "\n",
    "class CBOW(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "\n",
    "        #out: 1 x emdedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.activation_function1 = nn.ReLU()\n",
    "        \n",
    "        #out: 1 x vocab_size\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        self.activation_function2 = nn.LogSoftmax()\n",
    "        \n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # i believe .view() is useless here because the sum already produces a 1xEMB_DIM vector\n",
    "        embeds = sum(self.embeddings(inputs)).view(1,-1)\n",
    "        out = self.linear1(embeds)\n",
    "        out = self.activation_function1(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.activation_function2(out)\n",
    "        return out\n",
    "\n",
    "    def get_word_emdedding(self, word):\n",
    "        word = Variable(torch.LongTensor([w2i[word]]))\n",
    "        return self.embeddings(word).view(1,-1)\n",
    "\n",
    "\n",
    "def make_context_vector(context, w2i):\n",
    "    idxs = [w2i[w] for w in context]\n",
    "    tensor = torch.LongTensor(idxs)\n",
    "    return autograd.Variable(tensor)\n",
    "\n",
    "def sample_weights(model, n):\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        if i == n:\n",
    "            print(param[0])\n",
    "        \n",
    "\n",
    "def train(model, data):\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        total_loss = 0\n",
    "        for i, observation in enumerate(data):\n",
    "            dialog = observation['dialog']\n",
    "            target = observation['target']\n",
    "            \n",
    "            # Converts input and target to tensors\n",
    "            context_vec = make_context_vector(dialog, w2i)\n",
    "            target = Variable(torch.LongTensor([target.data]))\n",
    "            \n",
    "            # Zero out gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            log_probs = model(context_vec)\n",
    "    \n",
    "            # Calculate loss and update gradients\n",
    "            loss = loss_function(log_probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.data\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Finished epoch\", epoch, \"loss:\", total_loss)\n",
    "            sample_weights(model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_EASY = ['Data', 'sample_easy.json']\n",
    "TRAIN_EASY = ['Data', 'Easy', 'IR_train_easy.json']\n",
    "EMBEDDING_DIM = 5\n",
    "CONTEXT_SIZE = 2\n",
    "FREQ_THRESHOLD = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dialog_data = DialogDataset(os.path.join(*SAMPLE_EASY))\n",
    "\n",
    "w2i = {word : i for i, word in enumerate(dialog_data.vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text feature: \n",
      "\n",
      " Variable containing:\n",
      "\n",
      "Columns 0 to 7 \n",
      "-10.6338 -10.1004 -11.8775 -10.6243 -10.4888 -12.0977  -8.0102  -6.8183\n",
      "\n",
      "Columns 8 to 15 \n",
      " -3.1825  -4.6175  -9.2084 -16.5549 -13.4291 -14.3406  -6.9950  -8.0047\n",
      "\n",
      "Columns 16 to 23 \n",
      " -9.2783 -12.0327 -11.1683  -7.1018 -11.7965  -9.6078 -11.3064 -10.3735\n",
      "\n",
      "Columns 24 to 31 \n",
      " -8.1839 -12.0130 -15.1411  -4.2859 -12.3538  -5.1868  -7.3269 -11.8197\n",
      "\n",
      "Columns 32 to 39 \n",
      "-10.5630 -11.7885  -8.7020 -13.0513 -11.3707  -6.8526  -7.0757 -16.1151\n",
      "\n",
      "Columns 40 to 47 \n",
      "-10.3158  -6.4633  -9.9530  -7.1912  -6.8112  -9.7762  -7.8742  -6.6350\n",
      "\n",
      "Columns 48 to 55 \n",
      " -7.6338  -7.3580 -13.7431  -9.9217  -8.1045 -11.2687  -7.2950  -8.1376\n",
      "\n",
      "Columns 56 to 63 \n",
      "-12.7855 -12.6861 -16.0727  -1.6448  -6.8837 -11.1470 -10.0113 -12.5082\n",
      "\n",
      "Columns 64 to 71 \n",
      " -9.2732  -7.5832  -8.4038 -15.3918 -17.4359 -11.8781  -7.9767  -6.4230\n",
      "\n",
      "Columns 72 to 79 \n",
      "-12.2951  -7.3652 -14.8451 -14.0839  -9.0215 -15.5635 -12.9066 -12.0712\n",
      "\n",
      "Columns 80 to 87 \n",
      "-10.7096  -8.7530 -10.2872  -5.9924  -5.2413  -7.1296  -6.7896  -8.1038\n",
      "\n",
      "Columns 88 to 95 \n",
      " -7.3905 -10.8168  -9.9111  -6.1145 -10.0838 -11.1935  -3.9310 -13.3214\n",
      "\n",
      "Columns 96 to 103 \n",
      " -9.0309  -6.4020  -9.1090  -6.6488  -6.9668  -7.7798  -6.3579  -7.3810\n",
      "\n",
      "Columns 104 to 111 \n",
      "-13.2719  -9.1269  -5.5637  -9.2261 -11.7560 -14.0037  -7.5345 -13.1469\n",
      "\n",
      "Columns 112 to 119 \n",
      "-17.2366 -10.1706  -5.8953  -1.5316 -10.2053 -12.1434  -4.5005  -9.8019\n",
      "\n",
      "Columns 120 to 127 \n",
      " -9.0684 -10.4603 -13.0668  -8.8378  -9.8896  -9.7762  -7.1318  -3.0060\n",
      "\n",
      "Columns 128 to 135 \n",
      "-12.1798 -10.2047  -9.6583  -7.0928  -9.9998 -10.4180  -9.0965 -13.6114\n",
      "\n",
      "Columns 136 to 143 \n",
      "-11.5178 -10.7994  -9.7317  -1.6912 -13.7683 -12.3723  -4.4396 -10.8900\n",
      "\n",
      "Columns 144 to 151 \n",
      " -7.3635 -12.7958 -10.1162  -7.7622  -8.8112  -8.9082  -5.1424  -6.2589\n",
      "\n",
      "Columns 152 to 159 \n",
      "-10.1765 -10.1191  -8.4763 -12.3244 -11.6317 -13.2461  -7.1632 -10.5642\n",
      "\n",
      "Columns 160 to 162 \n",
      " -1.6926 -11.8085 -12.4263\n",
      "[torch.FloatTensor of size 1x163]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(len(w2i), EMBEDDING_DIM)\n",
    "\n",
    "train(model, dialog_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
